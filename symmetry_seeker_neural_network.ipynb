{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "O7EggnQhnVkK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p3Z_HuCBPRy",
        "outputId": "1643843d-2f32-49df-ce27-bb72c7612ce1"
      },
      "source": [
        "#Your google drive is made accesible to Colab.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd /content/drive/MyDrive/TESIS_AI_PABLO/Codes_data_paper_symmetries/\n",
        "    %ls -lht\n",
        "    # To import own packages set local path in packages syspath\n",
        "    import sys\n",
        "    sys.path.insert(0,\"./\")\n",
        "except ImportError:\n",
        "    print(\"You are not in google.colab!!\")\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/TESIS_AI_PABLO/Codes_data_paper_symmetries\n",
            "total 345K\n",
            "-rw------- 1 root root 202K Jul 13 13:17  symmetry_seeker_neural_network.ipynb\n",
            "drwx------ 2 root root 4.0K Jul  5 15:46  \u001b[0m\u001b[01;34mmodels_3root\u001b[0m/\n",
            "drwx------ 2 root root 4.0K Jul  5 15:43  \u001b[01;34mmodels_3root_pablo_\u001b[0m/\n",
            "-rw------- 1 root root 2.5K Jul  5 15:41  plot_delta_loss_vs_M.py\n",
            "-rw------- 1 root root 6.2K Jul  5 15:26  calculate_group_metrics.py\n",
            "-rw------- 1 root root  13K Jul  5 15:26  subroutines.py\n",
            "-rw------- 1 root root  78K Jul  4 19:01 'explicación carpeta códigos + datos paper simetrías.pdf'\n",
            "drwx------ 2 root root 4.0K Jul  4 16:35  \u001b[01;34mdata\u001b[0m/\n",
            "drwx------ 2 root root 4.0K Jul  4 16:29  \u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw------- 1 root root 8.1K Jul  4 16:29  main_SSNN_dynamic_M.py\n",
            "drwx------ 2 root root 4.0K Jul  4 14:47  \u001b[01;34mmodels_tetrahedron\u001b[0m/\n",
            "drwx------ 2 root root 4.0K Jul  4 14:47  \u001b[01;34mmodels_square\u001b[0m/\n",
            "drwx------ 2 root root 4.0K Jul  4 14:47  \u001b[01;34mmodels_rectangle\u001b[0m/\n",
            "drwx------ 2 root root 4.0K Jul  4 14:47  \u001b[01;34mmodels_NP\u001b[0m/\n",
            "-rw------- 1 root root 3.0K Jul  4 12:37  generate_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main imports"
      ],
      "metadata": {
        "id": "sD3mQaex2x8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import Input,Dense\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "#holaholaa"
      ],
      "metadata": {
        "id": "HCuTD9gJ2wcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Symmetric Seeker Neural Network $\\rightarrow$ SymSeekerNN class"
      ],
      "metadata": {
        "id": "jswK_WdInX-p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvQ-paBYBR_q"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.python.keras.engine import compile_utils\n",
        "\n",
        "class SymSeekerNN(tf.keras.Model):\n",
        "    '''\n",
        "    Description\n",
        "    '''\n",
        "    def __init__(self,Nfeatures,Nbranches,\n",
        "                 input_layer=None,\n",
        "                 standard_layers=None,\n",
        "                 branches_output=None,\n",
        "                 model_saved=None):\n",
        "        super(SymSeekerNN, self).__init__()\n",
        "\n",
        "        self.Nfeatures=Nfeatures\n",
        "        self.Nbranches=Nbranches\n",
        "        #try... except\n",
        "        if(model_saved!=None):\n",
        "          self.model=model_saved\n",
        "        else:\n",
        "          self.input_layer=input_layer\n",
        "          self.standard_layers=standard_layers\n",
        "          # Latent layer\n",
        "          self.latent_layer = standard_layers.layers[-1]\n",
        "          self.latent_layer.units=(self.Nfeatures)\n",
        "          # Branches\n",
        "          self.branches_output = branches_output\n",
        "          self.branches_output.units=int(self.Nfeatures*self.Nbranches)\n",
        "          self.branches_output.use_bias= False\n",
        "          self.branches_output.activation=None\n",
        "          self.model=Sequential([self.input_layer,\n",
        "                                self.standard_layers,\n",
        "                                self.branches_output])\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, path,Nfeatures,Nbranches):\n",
        "        model_saved=models.load_model(path,compile=False)\n",
        "        # Create an instance of the custom model class using the provided config\n",
        "        model = cls(Nfeatures,Nbranches,model_saved=model_saved)\n",
        "        # Perform any additional customizations if required\n",
        "        return model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The usual forward pass of the model\n",
        "        return self.model(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SymSeekerNN, self).get_config()\n",
        "        # Add any custom configurations to the config dictionary\n",
        "        custom_methods = {\n",
        "            'summary' : self.summary.__name__,\n",
        "            'symmetry_loss_Nb'  : self.symmetry_loss_Nb.__name__,\n",
        "            'calculate_d_closed': self.calculate_d_closed.__name__,\n",
        "            'calculate_d_inv'   : self.calculate_d_inv.__name__,\n",
        "            'obtain_list_branch_matrices_ref' : self.obtain_list_branch_matrices_ref.__name__,\n",
        "            '_list_acting_branches' : self._list_acting_branches.__name__,\n",
        "            'activity_single_branch' : self.activity_single_branch.__name__,\n",
        "            'list_activities' : self.list_activities.__name__\n",
        "        }\n",
        "        config['custom_methods'] = custom_methods\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Create an instance of the custom model class using the provided config\n",
        "        model = cls()\n",
        "        # Perform any additional customizations if required\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "       return self.model.summary()\n",
        "\n",
        "    def evaluate_loss(self,x_true,x_pred):\n",
        "       return self.loss(x_true,x_pred)\n",
        "\n",
        "    def compile(self, optimizer='adam',**kwargs):\n",
        "        super(SymSeekerNN, self).compile(**kwargs)\n",
        "        self.optimizer= optimizer\n",
        "        # Uses symmetry_loss_Nb\n",
        "        self.loss=self.symmetry_loss_Nb(self.Nbranches,self.Nfeatures)\n",
        "        loss_weights=None\n",
        "        self.output_names = None\n",
        "        self.model.compile(loss=self.loss)\n",
        "        self.compiled_loss = compile_utils.LossesContainer(\n",
        "          self.loss, loss_weights, output_names=self.output_names)\n",
        "\n",
        "    def symmetry_loss_Nb(self,Nbranches,Nfeatures):\n",
        "      '''\n",
        "      description\n",
        "      '''\n",
        "      def NBsym_loss(x_true,x_pred):\n",
        "\n",
        "          mbs = tf.shape(x_pred)[0]\n",
        "\n",
        "          x_true = tf.cast(x_true, dtype = 'float32')\n",
        "          x_pred = tf.cast(x_pred, dtype = 'float32')\n",
        "\n",
        "          diff_squared = (x_pred-x_true)*(x_pred-x_true)\n",
        "          sum_kernel = tf.constant(np.ones((1,Nfeatures)),tf.float32)\n",
        "          cost_matrix = tf.nn.conv2d(tf.reshape(diff_squared,(1,mbs,int(Nfeatures*Nbranches),1)),tf.reshape(sum_kernel,(1,Nfeatures,1,1)),strides=[1,Nfeatures],padding='VALID')\n",
        "          cost_matrix = tf.reshape(cost_matrix,(mbs,Nbranches))\n",
        "\n",
        "          cost_matrix = 0.5*cost_matrix\n",
        "          cost_vector = tf.math.reduce_min(cost_matrix, axis=1)\n",
        "          mbs = tf.cast(mbs, dtype = 'float32')\n",
        "          loss = tf.math.reduce_sum(cost_vector)/mbs\n",
        "          return loss\n",
        "      return NBsym_loss\n",
        "\n",
        "    def obtain_list_branch_matrices_ref(self,index_reference):\n",
        "      '''\n",
        "      index_reference goes from 0 to Nbranches-1\n",
        "      '''\n",
        "      w = self.model.get_weights()\n",
        "      w_ref_inv = np.linalg.inv(w[-1][0:self.Nfeatures,index_reference*self.Nfeatures:(index_reference+1)*self.Nfeatures])\n",
        "      list_mats = []\n",
        "      for branch in range(self.Nbranches):\n",
        "          real_branch = w[-1][0:self.Nfeatures,branch*self.Nfeatures:(branch+1)*self.Nfeatures]\n",
        "          list_mats.append( np.matmul(w_ref_inv,real_branch) )\n",
        "      return list_mats\n",
        "\n",
        "    def calculate_d_closed(self,index_reference):\n",
        "      list_mats=self.obtain_list_branch_matrices_ref(index_reference)\n",
        "      sum = 0.0\n",
        "      for alpha in list_mats:\n",
        "          for beta in list_mats:\n",
        "              Wab = np.matmul(alpha,beta)\n",
        "              min = 100000000000000.0\n",
        "              for gamma in list_mats:\n",
        "                  Wab_Wg = Wab - gamma\n",
        "                  loss = np.sum( np.absolute( Wab_Wg ) )\n",
        "                  if(loss<min):\n",
        "                      min = loss\n",
        "              sum = sum + min\n",
        "      d_closed = sum/(self.Nfeatures*self.Nfeatures*self.Nbranches*self.Nbranches)\n",
        "      return d_closed\n",
        "\n",
        "    def calculate_d_inv(self,index_reference):\n",
        "      '''\n",
        "      Eq. XX in Calvo-Barlés et al. ....\n",
        "      '''\n",
        "      list_mats=self.obtain_list_branch_matrices_ref(index_reference)\n",
        "      sum2 = 0.0\n",
        "      for alpha in list_mats:\n",
        "          Wa_inv = np.linalg.inv(alpha)\n",
        "          min2 = 100000000000000.0\n",
        "          for gamma in list_mats:\n",
        "              Wa_inv_Wg = Wa_inv - gamma\n",
        "              loss2 = np.sum( np.absolute (Wa_inv_Wg ) )\n",
        "              if(loss2 < min2):\n",
        "                  min2=loss2\n",
        "          sum2 = sum2 + min2\n",
        "      d_inv = sum2/(self.Nfeatures*self.Nfeatures*self.Nbranches)\n",
        "      return d_inv\n",
        "\n",
        "    def _list_acting_branches(self,x_true,x_pred):\n",
        "        '''\n",
        "        Returns a list of size ntrain that gives the branch index with the minimum MSE\n",
        "        '''\n",
        "        ndata = np.shape(x_true)[0]\n",
        "\n",
        "        cost_matrix = (x_pred[:,0]-x_true[:,0])*(x_pred[:,0]-x_true[:,0])\n",
        "        for j in range(1,self.Nfeatures):\n",
        "            cost_matrix = cost_matrix + (x_pred[:,j]-x_true[:,j])*(x_pred[:,j]-x_true[:,j])\n",
        "\n",
        "        for i in range(1,self.Nbranches):\n",
        "            cost_column = (x_pred[:,self.Nfeatures*i]-x_true[:,0])*(x_pred[:,self.Nfeatures*i]-x_true[:,0])\n",
        "            for j in range(1,self.Nfeatures):\n",
        "                cost_column = cost_column + (x_pred[:,self.Nfeatures*i+j]-x_true[:,j])*(x_pred[:,self.Nfeatures*i+j]-x_true[:,j])\n",
        "\n",
        "            cost_matrix = np.concatenate((cost_matrix,cost_column), axis = 0)\n",
        "\n",
        "        cost_matrix = np.reshape(cost_matrix,(self.Nbranches,ndata))\n",
        "        cost_matrix = np.transpose(cost_matrix)\n",
        "        cost_matrix = 0.5*cost_matrix\n",
        "        return np.argmin(cost_matrix,axis=1)\n",
        "\n",
        "    def activity_single_branch(self,branch_to_evaluate,x_true,x_pred):\n",
        "        '''\n",
        "        Returns the activity of the branch with index \"branch_to_evaluate\"\n",
        "        '''\n",
        "        ntrain = np.shape(x_true)[0]\n",
        "        ##### CALCULATE column_mins #####\n",
        "        column_mins = self._list_acting_branches(x_true,x_pred)\n",
        "        column_mins = list(column_mins)\n",
        "        ##### CALCULATE list_counter #####\n",
        "        list_percentage = []\n",
        "        for i in range(self.Nbranches):\n",
        "            list_percentage.append((column_mins.count(i)*100)/ntrain)\n",
        "        return list_percentage[branch_to_evaluate]\n",
        "\n",
        "    def list_activities(self,x_true,x_pred):\n",
        "        '''\n",
        "        Returns the list of activities of all the branches\n",
        "        '''\n",
        "        ntrain = np.shape(x_true)[0]\n",
        "        ##### CALCULATE column_mins #####\n",
        "        column_mins = self._list_acting_branches(x_true,x_pred)\n",
        "        column_mins = list(column_mins)\n",
        "        ##### CALCULATE list_counter #####\n",
        "        list_percentage = []\n",
        "        for i in range(self.Nbranches):\n",
        "            list_percentage.append((column_mins.count(i)*100)/ntrain)\n",
        "        return list_percentage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SymSeekNN Class information"
      ],
      "metadata": {
        "id": "58jfJn7BILJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class class information\n",
        "print(SymSeekerNN.__doc__)\n",
        "\n",
        "# Methods available\n",
        "method_list = [method for method in dir(SymSeekerNN) if method.startswith('__') is False]\n",
        "print(\"Methods defined for class \",SymSeekerNN.__name__)\n",
        "\n",
        "for method in method_list:\n",
        "  print(method)\n",
        "print('\\n')\n",
        "\n",
        "# Description of the method (if available)\n",
        "# From the list below write print(SymSeekerNN.name_of_the_method.__doc__)\n",
        "# For example\n",
        "print(\"Method=\",SymSeekerNN.symmetry_loss_Nb.__name__)\n",
        "print(SymSeekerNN.symmetry_loss_Nb.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyWugMTmDv__",
        "outputId": "ee584789-495a-43bd-9dd0-33190cf2662e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Description\n",
            "    \n",
            "Methods defined for class  SymSeekerNN\n",
            "_SCALAR_UPRANKING_ON\n",
            "_TF_MODULE_IGNORED_PROPERTIES\n",
            "_add_trackable\n",
            "_add_trackable_child\n",
            "_add_variable_with_custom_getter\n",
            "_assert_compile_was_called\n",
            "_assert_weights_created\n",
            "_autographed_call\n",
            "_cast_single_input\n",
            "_check_call_args\n",
            "_check_sample_weight_warning\n",
            "_checkpoint_dependencies\n",
            "_clear_losses\n",
            "_compile_was_called\n",
            "_compute_dtype\n",
            "_configure_steps_per_execution\n",
            "_dedup_weights\n",
            "_deferred_dependencies\n",
            "_delete_tracking\n",
            "_deserialization_dependencies\n",
            "_deserialize_from_proto\n",
            "_dtype\n",
            "_eager_losses\n",
            "_expects_mask_arg\n",
            "_expects_training_arg\n",
            "_export_to_saved_model_graph\n",
            "_flatten\n",
            "_flatten_layers\n",
            "_flatten_modules\n",
            "_functional_construction_call\n",
            "_gather_children_attribute\n",
            "_gather_saveables_for_checkpoint\n",
            "_get_callback_model\n",
            "_get_cell_name\n",
            "_get_compile_args\n",
            "_get_existing_metric\n",
            "_get_input_masks\n",
            "_get_node_attribute_at_index\n",
            "_get_optimizer\n",
            "_get_save_spec\n",
            "_get_trainable_state\n",
            "_get_unnested_name_scope\n",
            "_handle_activity_regularization\n",
            "_handle_deferred_dependencies\n",
            "_handle_weight_regularization\n",
            "_in_multi_worker_mode\n",
            "_inbound_nodes\n",
            "_infer_output_signature\n",
            "_init_batch_counters\n",
            "_init_call_fn_args\n",
            "_init_set_name\n",
            "_instrument_layer_creation\n",
            "_is_layer\n",
            "_keras_api_names\n",
            "_keras_api_names_v1\n",
            "_keras_tensor_symbolic_call\n",
            "_list_acting_branches\n",
            "_load_own_variables\n",
            "_lookup_dependency\n",
            "_maybe_build\n",
            "_maybe_cast_inputs\n",
            "_maybe_create_attribute\n",
            "_maybe_initialize_trackable\n",
            "_maybe_load_initial_counters_from_ckpt\n",
            "_must_restore_from_config\n",
            "_name_based_attribute_restore\n",
            "_name_based_restores\n",
            "_name_scope\n",
            "_no_dependency\n",
            "_obj_reference_counts\n",
            "_object_identifier\n",
            "_outbound_nodes\n",
            "_preload_simple_restoration\n",
            "_reset_compile_cache\n",
            "_restore_from_tensors\n",
            "_save_experimental\n",
            "_save_own_variables\n",
            "_serialize_to_proto\n",
            "_serialize_to_tensors\n",
            "_set_connectivity_metadata\n",
            "_set_dtype_policy\n",
            "_set_inputs\n",
            "_set_mask_keras_history_checked\n",
            "_set_mask_metadata\n",
            "_set_save_spec\n",
            "_set_trainable_state\n",
            "_set_training_mode\n",
            "_setattr_tracking\n",
            "_should_cast_single_input\n",
            "_should_compute_mask\n",
            "_should_eval\n",
            "_tf_api_names\n",
            "_tf_api_names_v1\n",
            "_track_trackable\n",
            "_track_variable\n",
            "_track_variables\n",
            "_trackable_children\n",
            "_trackable_saved_model_saver\n",
            "_tracking_metadata\n",
            "_unconditional_checkpoint_dependencies\n",
            "_unconditional_dependency_names\n",
            "_undeduplicated_weights\n",
            "_update_trackables\n",
            "_update_uid\n",
            "_updated_config\n",
            "_use_input_spec_as_call_signature\n",
            "_validate_and_get_metrics_result\n",
            "_validate_compile\n",
            "_validate_target_and_loss\n",
            "activity_regularizer\n",
            "activity_single_branch\n",
            "add_loss\n",
            "add_metric\n",
            "add_update\n",
            "add_variable\n",
            "add_weight\n",
            "build\n",
            "build_from_config\n",
            "calculate_d_closed\n",
            "calculate_d_inv\n",
            "call\n",
            "compile\n",
            "compile_from_config\n",
            "compute_dtype\n",
            "compute_loss\n",
            "compute_mask\n",
            "compute_metrics\n",
            "compute_output_shape\n",
            "compute_output_signature\n",
            "count_params\n",
            "distribute_reduction_method\n",
            "distribute_strategy\n",
            "dtype\n",
            "dtype_policy\n",
            "dynamic\n",
            "evaluate\n",
            "evaluate_generator\n",
            "evaluate_loss\n",
            "export\n",
            "finalize_state\n",
            "fit\n",
            "fit_generator\n",
            "from_config\n",
            "get_build_config\n",
            "get_compile_config\n",
            "get_config\n",
            "get_input_at\n",
            "get_input_mask_at\n",
            "get_input_shape_at\n",
            "get_layer\n",
            "get_metrics_result\n",
            "get_output_at\n",
            "get_output_mask_at\n",
            "get_output_shape_at\n",
            "get_weight_paths\n",
            "get_weights\n",
            "inbound_nodes\n",
            "input\n",
            "input_mask\n",
            "input_shape\n",
            "input_spec\n",
            "jit_compile\n",
            "layers\n",
            "list_activities\n",
            "load_model\n",
            "load_weights\n",
            "losses\n",
            "make_predict_function\n",
            "make_test_function\n",
            "make_train_function\n",
            "metrics\n",
            "metrics_names\n",
            "name\n",
            "name_scope\n",
            "non_trainable_variables\n",
            "non_trainable_weights\n",
            "obtain_list_branch_matrices_ref\n",
            "outbound_nodes\n",
            "output\n",
            "output_mask\n",
            "output_shape\n",
            "predict\n",
            "predict_generator\n",
            "predict_on_batch\n",
            "predict_step\n",
            "reset_metrics\n",
            "reset_states\n",
            "run_eagerly\n",
            "save\n",
            "save_spec\n",
            "save_weights\n",
            "set_weights\n",
            "state_updates\n",
            "stateful\n",
            "submodules\n",
            "summary\n",
            "supports_masking\n",
            "symmetry_loss_Nb\n",
            "test_on_batch\n",
            "test_step\n",
            "to_json\n",
            "to_yaml\n",
            "train_on_batch\n",
            "train_step\n",
            "trainable\n",
            "trainable_variables\n",
            "trainable_weights\n",
            "updates\n",
            "variable_dtype\n",
            "variables\n",
            "weights\n",
            "with_name_scope\n",
            "\n",
            "\n",
            "Method= symmetry_loss_Nb\n",
            "\n",
            "      description\n",
            "      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the data"
      ],
      "metadata": {
        "id": "QXDMyXNgDyH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z_power_n(x,y,n):\n",
        "    z = x + 1j*y\n",
        "    w = z**n\n",
        "    return np.real(w),np.imag(w)\n",
        "\n",
        "Nsamples = 20000 #Number of samples for training/validation data\n",
        "L = 10.0\n",
        "n = 3\n",
        "problem_name = str(n)+'root'\n",
        "\n",
        "# GENERATE DATA\n",
        "x = [] #Physical features\n",
        "y = [] #Measurable magnitudes\n",
        "\n",
        "for i in range(Nsamples):\n",
        "    #print('Sample #',i)\n",
        "    #polar coordinates\n",
        "    rho = np.random.uniform(0,L)\n",
        "    phi = np.random.uniform(0,2*np.pi) #np.random.normal(np.pi/2, np.pi/4) (gaussian)\n",
        "    xx = rho*np.cos(phi); yy = rho*np.sin(phi)\n",
        "    single_x = [xx,yy]\n",
        "    single_y = z_power_n(xx,yy,n)\n",
        "    x.append(single_x)\n",
        "    y.append(single_y)\n",
        "x = np.array(x)  # w\n",
        "y = np.array(y)  # z^n"
      ],
      "metadata": {
        "id": "iK9PJa9wC3QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the data (¿Se dará una database para probar?)"
      ],
      "metadata": {
        "id": "LuEdRaEDDYK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def read_data(fileX, filey):\n",
        "    X = pd.read_csv(fileX, header = None)\n",
        "    X = np.array(X)\n",
        "    y = pd.read_csv(filey, header = None)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "filex = './data/X_3root.csv'\n",
        "filey = './data/Y_3root.csv'\n",
        "x,y = read_data(filex,filey)"
      ],
      "metadata": {
        "id": "nOTFw9ANDWeP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define tranining and validation dataset"
      ],
      "metadata": {
        "id": "ag2TSTLq2-qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntrain = int(0.95*Nsamples)\n",
        "nvalidation = Nsamples-ntrain\n",
        "\n",
        "Nfeatures = y.shape[1]\n",
        "L = 10.0\n",
        "n = 3\n",
        "x = x/L\n",
        "y = y/(L**n)\n",
        "problem_name = str(n)+'root'\n",
        "\n",
        "xtr_single = x[0:ntrain,:]\n",
        "xva_single = x[ntrain:ntrain + nvalidation, :]\n",
        "ytr = y[0:ntrain,:]\n",
        "yva = y[ntrain:ntrain + nvalidation, :]\n",
        "\n",
        "print(xtr_single.shape,xva_single.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB5aurY02-wX",
        "outputId": "aaae120f-b848-4eb3-8530-740fe7e559ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19000, 2) (1000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the SNNN network"
      ],
      "metadata": {
        "id": "T8CIqUk3BmR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "minib_size = 128\n",
        "eta = 0.01\n",
        "Nbranches = 3 #The training process starts with a NN with \"max_Nbranches\" branches\n",
        "\n",
        "# Save the models\n",
        "directory = './models_'+str(problem_name)+'/'\n",
        "\n",
        "# 0) EXPAND TRAINING OUTPUTS\n",
        "#ytr = np.tile(ytr_single,reps=(1,Nbranches))\n",
        "#yva = np.tile(yva_single,reps=(1,Nbranches))\n",
        "#output_neurons = ytr.shape[1]\n",
        "\n",
        "xtr = np.tile(xtr_single,reps=(1,Nbranches))\n",
        "xva = np.tile(xva_single,reps=(1,Nbranches))\n",
        "output_neurons = xtr.shape[1]\n",
        "\n",
        "# 1) DEFINE THE MODEL\n",
        "# Input layer\n",
        "input_neurons  = ytr.shape[1]\n",
        "input_layer = Input(shape=(input_neurons,))\n",
        "\n",
        "# Standard model\n",
        "nneurons = 10; nhidden = 2\n",
        "standard_layers=Sequential()\n",
        "hidden_layer = Dense(nneurons, activation='sigmoid' ,\n",
        "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1/np.sqrt(input_neurons)),\n",
        "                     bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1))\n",
        "standard_layers.add(hidden_layer)\n",
        "for j in range(1,nhidden):\n",
        "    hidden_layer = Dense(nneurons, activation='sigmoid' ,\n",
        "                         kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1/np.sqrt(nneurons)),\n",
        "                         bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1))\n",
        "    standard_layers.add(hidden_layer)\n",
        "\n",
        "# Latent layer\n",
        "latent_layer = Dense(1,activation=None,\n",
        "                     kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1/np.sqrt(nneurons)),\n",
        "                     bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1))\n",
        "\n",
        "standard_layers.add(latent_layer)\n",
        "\n",
        "# Branches\n",
        "branches_output = Dense(1,\n",
        "                        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1/np.sqrt(Nfeatures)))\n",
        "\n",
        "model=SymSeekerNN(Nfeatures,Nbranches,\n",
        "                  input_layer,\n",
        "                  standard_layers,\n",
        "                  branches_output)\n",
        "\n",
        "# 3) COMPILE THE MODEL\n",
        "name_opt='Adam'\n",
        "opt = tf.keras.optimizers.Adam(eta)\n",
        "model.compile(optimizer=opt)\n",
        "model.summary()\n",
        "\n",
        "# 3) FIT THE MODEL\n",
        "r = model.fit(ytr, xtr, batch_size = minib_size, epochs=epochs,verbose=1, validation_data=(yva, xva))\n",
        "\n",
        "# 4) SAVE THE MODEL\n",
        "i=0\n",
        "file_name = 'Model_'+str(i)+'sim_'+str(Nbranches)+'branches'\n",
        "model.save(directory+'/'+file_name)\n",
        "\n",
        "# 5) PLOT THE LOSS\n",
        "plt.plot(r.history['loss'],label='loss')\n",
        "plt.plot(r.history['val_loss'],label='val loss')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SP6WME9kWCeN",
        "outputId": "c9a7a1df-411b-4e80-ffd1-4b76aa5a1345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_1 (Sequential)   (None, 2)                 162       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 6)                 12        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174\n",
            "Trainable params: 174\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "149/149 [==============================] - 3s 7ms/step - loss: 0.1001 - val_loss: 0.0712\n",
            "Epoch 2/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0598 - val_loss: 0.0502\n",
            "Epoch 3/1000\n",
            "149/149 [==============================] - 2s 10ms/step - loss: 0.0351 - val_loss: 0.0226\n",
            "Epoch 4/1000\n",
            "149/149 [==============================] - 1s 7ms/step - loss: 0.0192 - val_loss: 0.0170\n",
            "Epoch 5/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.0136\n",
            "Epoch 6/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0108\n",
            "Epoch 7/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0094\n",
            "Epoch 8/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0088\n",
            "Epoch 9/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0077\n",
            "Epoch 10/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0073\n",
            "Epoch 11/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0065\n",
            "Epoch 12/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0062\n",
            "Epoch 13/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0060\n",
            "Epoch 14/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0056\n",
            "Epoch 15/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0052\n",
            "Epoch 16/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0049\n",
            "Epoch 17/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 18/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 19/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 20/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 21/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0039\n",
            "Epoch 22/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 23/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 24/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0039\n",
            "Epoch 25/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
            "Epoch 26/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 27/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 28/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 0.0035\n",
            "Epoch 29/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 30/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 31/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0030 - val_loss: 0.0032\n",
            "Epoch 32/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0030 - val_loss: 0.0032\n",
            "Epoch 33/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 34/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 35/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 36/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 37/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 38/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 39/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0028\n",
            "Epoch 40/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 41/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 42/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 43/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 44/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 45/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0031\n",
            "Epoch 46/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 47/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 48/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 49/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 50/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 51/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 52/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 53/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 54/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 55/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 56/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 57/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 58/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 59/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 60/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 61/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0026\n",
            "Epoch 62/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 63/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 64/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 65/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 66/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 67/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 68/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 69/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 70/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 71/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 72/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 73/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 74/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 75/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 76/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 77/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 78/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 79/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.0019\n",
            "Epoch 80/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0018\n",
            "Epoch 81/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 82/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 83/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 84/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 85/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 86/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 87/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 88/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 89/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 90/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 91/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 92/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 93/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 94/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 95/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 96/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 97/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 98/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 99/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 100/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 101/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 102/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 103/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 104/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 105/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 106/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 107/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 108/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 109/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 110/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 111/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 112/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 113/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 114/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 115/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 116/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 117/1000\n",
            "149/149 [==============================] - 1s 8ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 118/1000\n",
            "149/149 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 119/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 120/1000\n",
            "149/149 [==============================] - 1s 6ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 121/1000\n",
            "149/149 [==============================] - 1s 8ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 122/1000\n",
            "149/149 [==============================] - 2s 15ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 123/1000\n",
            "149/149 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 124/1000\n",
            "149/149 [==============================] - 1s 6ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 125/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 126/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 127/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 128/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 129/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 130/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 131/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 132/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 133/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 134/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 135/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 136/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.8457e-04 - val_loss: 0.0013\n",
            "Epoch 137/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.9390e-04 - val_loss: 0.0013\n",
            "Epoch 138/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.8056e-04 - val_loss: 0.0013\n",
            "Epoch 139/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 140/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.6763e-04 - val_loss: 0.0013\n",
            "Epoch 141/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.7493e-04 - val_loss: 0.0013\n",
            "Epoch 142/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.6333e-04 - val_loss: 0.0012\n",
            "Epoch 143/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.6855e-04 - val_loss: 0.0013\n",
            "Epoch 144/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.7302e-04 - val_loss: 0.0011\n",
            "Epoch 145/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.5320e-04 - val_loss: 0.0011\n",
            "Epoch 146/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.5018e-04 - val_loss: 0.0013\n",
            "Epoch 147/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 9.2412e-04 - val_loss: 0.0012\n",
            "Epoch 148/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 9.3691e-04 - val_loss: 0.0011\n",
            "Epoch 149/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 9.1457e-04 - val_loss: 0.0011\n",
            "Epoch 150/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 9.2213e-04 - val_loss: 0.0014\n",
            "Epoch 151/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 9.2973e-04 - val_loss: 0.0012\n",
            "Epoch 152/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 9.1159e-04 - val_loss: 0.0012\n",
            "Epoch 153/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 9.1173e-04 - val_loss: 0.0012\n",
            "Epoch 154/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 9.0832e-04 - val_loss: 0.0011\n",
            "Epoch 155/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.0384e-04 - val_loss: 0.0011\n",
            "Epoch 156/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.8639e-04 - val_loss: 9.7437e-04\n",
            "Epoch 157/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.7824e-04 - val_loss: 0.0012\n",
            "Epoch 158/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.9559e-04 - val_loss: 0.0011\n",
            "Epoch 159/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.9319e-04 - val_loss: 0.0011\n",
            "Epoch 160/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.1364e-04 - val_loss: 0.0012\n",
            "Epoch 161/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.9606e-04 - val_loss: 0.0011\n",
            "Epoch 162/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.5629e-04 - val_loss: 0.0011\n",
            "Epoch 163/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 9.1652e-04 - val_loss: 0.0012\n",
            "Epoch 164/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.7118e-04 - val_loss: 0.0011\n",
            "Epoch 165/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.3629e-04 - val_loss: 0.0011\n",
            "Epoch 166/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.4473e-04 - val_loss: 0.0012\n",
            "Epoch 167/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.5493e-04 - val_loss: 0.0012\n",
            "Epoch 168/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.3502e-04 - val_loss: 0.0012\n",
            "Epoch 169/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.4219e-04 - val_loss: 0.0011\n",
            "Epoch 170/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.2622e-04 - val_loss: 0.0011\n",
            "Epoch 171/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.5354e-04 - val_loss: 0.0011\n",
            "Epoch 172/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.2783e-04 - val_loss: 0.0011\n",
            "Epoch 173/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.2361e-04 - val_loss: 0.0013\n",
            "Epoch 174/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.4482e-04 - val_loss: 0.0011\n",
            "Epoch 175/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.2271e-04 - val_loss: 0.0010\n",
            "Epoch 176/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.2010e-04 - val_loss: 0.0011\n",
            "Epoch 177/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.1153e-04 - val_loss: 0.0011\n",
            "Epoch 178/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 8.2934e-04 - val_loss: 0.0011\n",
            "Epoch 179/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 8.0666e-04 - val_loss: 0.0011\n",
            "Epoch 180/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.9143e-04 - val_loss: 0.0011\n",
            "Epoch 181/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 7.9324e-04 - val_loss: 0.0011\n",
            "Epoch 182/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 8.0108e-04 - val_loss: 0.0011\n",
            "Epoch 183/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.7905e-04 - val_loss: 0.0011\n",
            "Epoch 184/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 8.1248e-04 - val_loss: 0.0011\n",
            "Epoch 185/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.9407e-04 - val_loss: 9.9245e-04\n",
            "Epoch 186/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.8589e-04 - val_loss: 0.0011\n",
            "Epoch 187/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.7449e-04 - val_loss: 0.0010\n",
            "Epoch 188/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.6808e-04 - val_loss: 9.8695e-04\n",
            "Epoch 189/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.7727e-04 - val_loss: 0.0010\n",
            "Epoch 190/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.8637e-04 - val_loss: 0.0010\n",
            "Epoch 191/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.6594e-04 - val_loss: 0.0010\n",
            "Epoch 192/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.5280e-04 - val_loss: 0.0010\n",
            "Epoch 193/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.7764e-04 - val_loss: 0.0010\n",
            "Epoch 194/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.7760e-04 - val_loss: 0.0011\n",
            "Epoch 195/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.8153e-04 - val_loss: 0.0010\n",
            "Epoch 196/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.5461e-04 - val_loss: 0.0011\n",
            "Epoch 197/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.5774e-04 - val_loss: 9.8245e-04\n",
            "Epoch 198/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 7.4891e-04 - val_loss: 0.0011\n",
            "Epoch 199/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.6740e-04 - val_loss: 0.0011\n",
            "Epoch 200/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.4014e-04 - val_loss: 0.0010\n",
            "Epoch 201/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.4072e-04 - val_loss: 0.0011\n",
            "Epoch 202/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.4617e-04 - val_loss: 0.0010\n",
            "Epoch 203/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.2666e-04 - val_loss: 0.0010\n",
            "Epoch 204/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.2108e-04 - val_loss: 9.6910e-04\n",
            "Epoch 205/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.4206e-04 - val_loss: 0.0011\n",
            "Epoch 206/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.3819e-04 - val_loss: 9.4625e-04\n",
            "Epoch 207/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.3106e-04 - val_loss: 0.0011\n",
            "Epoch 208/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.2626e-04 - val_loss: 9.7018e-04\n",
            "Epoch 209/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.2926e-04 - val_loss: 9.4190e-04\n",
            "Epoch 210/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.1377e-04 - val_loss: 9.2185e-04\n",
            "Epoch 211/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.9963e-04 - val_loss: 0.0010\n",
            "Epoch 212/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.2146e-04 - val_loss: 9.8140e-04\n",
            "Epoch 213/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.4156e-04 - val_loss: 9.2192e-04\n",
            "Epoch 214/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.0336e-04 - val_loss: 0.0011\n",
            "Epoch 215/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 7.1798e-04 - val_loss: 9.9883e-04\n",
            "Epoch 216/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.0345e-04 - val_loss: 9.4106e-04\n",
            "Epoch 217/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.9361e-04 - val_loss: 0.0010\n",
            "Epoch 218/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 7.0899e-04 - val_loss: 0.0011\n",
            "Epoch 219/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 6.9717e-04 - val_loss: 9.5432e-04\n",
            "Epoch 220/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.9793e-04 - val_loss: 9.2736e-04\n",
            "Epoch 221/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.8093e-04 - val_loss: 9.1607e-04\n",
            "Epoch 222/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 7.0500e-04 - val_loss: 0.0010\n",
            "Epoch 223/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 7.0727e-04 - val_loss: 9.3549e-04\n",
            "Epoch 224/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7837e-04 - val_loss: 9.0202e-04\n",
            "Epoch 225/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7989e-04 - val_loss: 0.0010\n",
            "Epoch 226/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.8170e-04 - val_loss: 9.6307e-04\n",
            "Epoch 227/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.6574e-04 - val_loss: 8.7335e-04\n",
            "Epoch 228/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7184e-04 - val_loss: 9.0158e-04\n",
            "Epoch 229/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7642e-04 - val_loss: 9.8807e-04\n",
            "Epoch 230/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.6731e-04 - val_loss: 8.7696e-04\n",
            "Epoch 231/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.6397e-04 - val_loss: 8.8613e-04\n",
            "Epoch 232/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7907e-04 - val_loss: 9.1370e-04\n",
            "Epoch 233/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.4686e-04 - val_loss: 8.5092e-04\n",
            "Epoch 234/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7794e-04 - val_loss: 8.9584e-04\n",
            "Epoch 235/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.4119e-04 - val_loss: 8.3975e-04\n",
            "Epoch 236/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.5229e-04 - val_loss: 9.2222e-04\n",
            "Epoch 237/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.7585e-04 - val_loss: 0.0011\n",
            "Epoch 238/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.6822e-04 - val_loss: 0.0010\n",
            "Epoch 239/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.5627e-04 - val_loss: 9.0194e-04\n",
            "Epoch 240/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.5283e-04 - val_loss: 8.4943e-04\n",
            "Epoch 241/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.4521e-04 - val_loss: 8.8810e-04\n",
            "Epoch 242/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.4287e-04 - val_loss: 9.8233e-04\n",
            "Epoch 243/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.2919e-04 - val_loss: 8.9542e-04\n",
            "Epoch 244/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.6062e-04 - val_loss: 8.7251e-04\n",
            "Epoch 245/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.3254e-04 - val_loss: 0.0011\n",
            "Epoch 246/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.5111e-04 - val_loss: 8.7688e-04\n",
            "Epoch 247/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.3453e-04 - val_loss: 9.0001e-04\n",
            "Epoch 248/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.2095e-04 - val_loss: 8.7933e-04\n",
            "Epoch 249/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 6.1921e-04 - val_loss: 8.7616e-04\n",
            "Epoch 250/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.3974e-04 - val_loss: 9.2123e-04\n",
            "Epoch 251/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 6.1860e-04 - val_loss: 8.7686e-04\n",
            "Epoch 252/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 6.3036e-04 - val_loss: 9.1944e-04\n",
            "Epoch 253/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.4449e-04 - val_loss: 8.5392e-04\n",
            "Epoch 254/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 6.2550e-04 - val_loss: 8.4830e-04\n",
            "Epoch 255/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.2964e-04 - val_loss: 8.8184e-04\n",
            "Epoch 256/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.2001e-04 - val_loss: 8.5364e-04\n",
            "Epoch 257/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.2514e-04 - val_loss: 8.4886e-04\n",
            "Epoch 258/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.0981e-04 - val_loss: 8.4902e-04\n",
            "Epoch 259/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.1903e-04 - val_loss: 9.0576e-04\n",
            "Epoch 260/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.1395e-04 - val_loss: 9.1960e-04\n",
            "Epoch 261/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.4662e-04 - val_loss: 8.3572e-04\n",
            "Epoch 262/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.1508e-04 - val_loss: 8.9001e-04\n",
            "Epoch 263/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.3320e-04 - val_loss: 9.3272e-04\n",
            "Epoch 264/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.2986e-04 - val_loss: 9.6588e-04\n",
            "Epoch 265/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.9819e-04 - val_loss: 8.5268e-04\n",
            "Epoch 266/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.1133e-04 - val_loss: 8.6135e-04\n",
            "Epoch 267/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.1083e-04 - val_loss: 8.3237e-04\n",
            "Epoch 268/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.8967e-04 - val_loss: 8.2059e-04\n",
            "Epoch 269/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.8923e-04 - val_loss: 8.3730e-04\n",
            "Epoch 270/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.9450e-04 - val_loss: 8.6757e-04\n",
            "Epoch 271/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.0537e-04 - val_loss: 7.9744e-04\n",
            "Epoch 272/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.9989e-04 - val_loss: 8.3222e-04\n",
            "Epoch 273/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.9904e-04 - val_loss: 0.0010\n",
            "Epoch 274/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 6.0169e-04 - val_loss: 8.1244e-04\n",
            "Epoch 275/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.8916e-04 - val_loss: 8.8060e-04\n",
            "Epoch 276/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.0365e-04 - val_loss: 7.9791e-04\n",
            "Epoch 277/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.9906e-04 - val_loss: 9.5385e-04\n",
            "Epoch 278/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.0449e-04 - val_loss: 8.5161e-04\n",
            "Epoch 279/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.8215e-04 - val_loss: 8.2813e-04\n",
            "Epoch 280/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.9414e-04 - val_loss: 8.4079e-04\n",
            "Epoch 281/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.6925e-04 - val_loss: 7.9523e-04\n",
            "Epoch 282/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 5.7902e-04 - val_loss: 7.4895e-04\n",
            "Epoch 283/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.7852e-04 - val_loss: 8.2397e-04\n",
            "Epoch 284/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.8443e-04 - val_loss: 8.4302e-04\n",
            "Epoch 285/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.9065e-04 - val_loss: 8.2918e-04\n",
            "Epoch 286/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.7150e-04 - val_loss: 9.4199e-04\n",
            "Epoch 287/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.7603e-04 - val_loss: 8.9168e-04\n",
            "Epoch 288/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.7285e-04 - val_loss: 9.3315e-04\n",
            "Epoch 289/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.8418e-04 - val_loss: 8.0339e-04\n",
            "Epoch 290/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.9486e-04 - val_loss: 8.2492e-04\n",
            "Epoch 291/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.7075e-04 - val_loss: 9.0571e-04\n",
            "Epoch 292/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.6999e-04 - val_loss: 8.4090e-04\n",
            "Epoch 293/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.7287e-04 - val_loss: 8.4549e-04\n",
            "Epoch 294/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.8462e-04 - val_loss: 7.9739e-04\n",
            "Epoch 295/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 6.0401e-04 - val_loss: 8.1193e-04\n",
            "Epoch 296/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.4696e-04 - val_loss: 8.7171e-04\n",
            "Epoch 297/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4950e-04 - val_loss: 8.0514e-04\n",
            "Epoch 298/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.6070e-04 - val_loss: 8.4390e-04\n",
            "Epoch 299/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.5837e-04 - val_loss: 7.8611e-04\n",
            "Epoch 300/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.5389e-04 - val_loss: 8.2939e-04\n",
            "Epoch 301/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.5980e-04 - val_loss: 7.9201e-04\n",
            "Epoch 302/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.7333e-04 - val_loss: 8.2607e-04\n",
            "Epoch 303/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.4670e-04 - val_loss: 7.8879e-04\n",
            "Epoch 304/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.5497e-04 - val_loss: 8.0973e-04\n",
            "Epoch 305/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4324e-04 - val_loss: 8.0817e-04\n",
            "Epoch 306/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.5189e-04 - val_loss: 8.3829e-04\n",
            "Epoch 307/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.5287e-04 - val_loss: 8.4694e-04\n",
            "Epoch 308/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4848e-04 - val_loss: 8.0496e-04\n",
            "Epoch 309/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.6966e-04 - val_loss: 7.5546e-04\n",
            "Epoch 310/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4144e-04 - val_loss: 8.6204e-04\n",
            "Epoch 311/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4940e-04 - val_loss: 8.0581e-04\n",
            "Epoch 312/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4074e-04 - val_loss: 7.4437e-04\n",
            "Epoch 313/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.5047e-04 - val_loss: 8.1590e-04\n",
            "Epoch 314/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.3567e-04 - val_loss: 7.4757e-04\n",
            "Epoch 315/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.4206e-04 - val_loss: 7.3994e-04\n",
            "Epoch 316/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 5.4119e-04 - val_loss: 8.0127e-04\n",
            "Epoch 317/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.5026e-04 - val_loss: 8.0452e-04\n",
            "Epoch 318/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.3675e-04 - val_loss: 7.6992e-04\n",
            "Epoch 319/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 5.3492e-04 - val_loss: 8.8398e-04\n",
            "Epoch 320/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.3690e-04 - val_loss: 7.8082e-04\n",
            "Epoch 321/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.4335e-04 - val_loss: 8.2204e-04\n",
            "Epoch 322/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 5.3519e-04 - val_loss: 8.2411e-04\n",
            "Epoch 323/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 5.3182e-04 - val_loss: 7.4455e-04\n",
            "Epoch 324/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 5.4158e-04 - val_loss: 8.0838e-04\n",
            "Epoch 325/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2692e-04 - val_loss: 7.1644e-04\n",
            "Epoch 326/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.2103e-04 - val_loss: 7.7029e-04\n",
            "Epoch 327/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.2140e-04 - val_loss: 7.8258e-04\n",
            "Epoch 328/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.1853e-04 - val_loss: 8.0691e-04\n",
            "Epoch 329/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.3385e-04 - val_loss: 7.8031e-04\n",
            "Epoch 330/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.3350e-04 - val_loss: 7.8089e-04\n",
            "Epoch 331/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.2591e-04 - val_loss: 7.5891e-04\n",
            "Epoch 332/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2221e-04 - val_loss: 7.9412e-04\n",
            "Epoch 333/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2585e-04 - val_loss: 7.8934e-04\n",
            "Epoch 334/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.1983e-04 - val_loss: 7.3826e-04\n",
            "Epoch 335/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.0664e-04 - val_loss: 8.3674e-04\n",
            "Epoch 336/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.2738e-04 - val_loss: 7.8345e-04\n",
            "Epoch 337/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.3492e-04 - val_loss: 7.6465e-04\n",
            "Epoch 338/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.0843e-04 - val_loss: 7.2154e-04\n",
            "Epoch 339/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.2052e-04 - val_loss: 7.8621e-04\n",
            "Epoch 340/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 5.1038e-04 - val_loss: 7.6584e-04\n",
            "Epoch 341/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.9927e-04 - val_loss: 6.9705e-04\n",
            "Epoch 342/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0744e-04 - val_loss: 7.8336e-04\n",
            "Epoch 343/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0021e-04 - val_loss: 7.0496e-04\n",
            "Epoch 344/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2457e-04 - val_loss: 7.0199e-04\n",
            "Epoch 345/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2471e-04 - val_loss: 8.0302e-04\n",
            "Epoch 346/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.1463e-04 - val_loss: 8.0840e-04\n",
            "Epoch 347/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0820e-04 - val_loss: 7.4973e-04\n",
            "Epoch 348/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0308e-04 - val_loss: 7.2129e-04\n",
            "Epoch 349/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2873e-04 - val_loss: 7.0207e-04\n",
            "Epoch 350/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.8998e-04 - val_loss: 7.0270e-04\n",
            "Epoch 351/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0331e-04 - val_loss: 7.5540e-04\n",
            "Epoch 352/1000\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 5.0559e-04 - val_loss: 7.0984e-04\n",
            "Epoch 353/1000\n",
            "149/149 [==============================] - 3s 22ms/step - loss: 4.9572e-04 - val_loss: 7.5890e-04\n",
            "Epoch 354/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 5.1802e-04 - val_loss: 7.2856e-04\n",
            "Epoch 355/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.2095e-04 - val_loss: 7.5233e-04\n",
            "Epoch 356/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0765e-04 - val_loss: 8.0947e-04\n",
            "Epoch 357/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0888e-04 - val_loss: 7.0891e-04\n",
            "Epoch 358/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9303e-04 - val_loss: 6.9581e-04\n",
            "Epoch 359/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8984e-04 - val_loss: 7.4868e-04\n",
            "Epoch 360/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9913e-04 - val_loss: 7.3499e-04\n",
            "Epoch 361/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8007e-04 - val_loss: 7.4712e-04\n",
            "Epoch 362/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.0465e-04 - val_loss: 7.9469e-04\n",
            "Epoch 363/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8330e-04 - val_loss: 7.3628e-04\n",
            "Epoch 364/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8299e-04 - val_loss: 7.5392e-04\n",
            "Epoch 365/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 5.1573e-04 - val_loss: 7.4408e-04\n",
            "Epoch 366/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9124e-04 - val_loss: 7.1339e-04\n",
            "Epoch 367/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9727e-04 - val_loss: 7.7770e-04\n",
            "Epoch 368/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8970e-04 - val_loss: 6.7209e-04\n",
            "Epoch 369/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9539e-04 - val_loss: 7.5594e-04\n",
            "Epoch 370/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7617e-04 - val_loss: 7.3485e-04\n",
            "Epoch 371/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9530e-04 - val_loss: 7.5001e-04\n",
            "Epoch 372/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9276e-04 - val_loss: 7.2621e-04\n",
            "Epoch 373/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8728e-04 - val_loss: 6.9361e-04\n",
            "Epoch 374/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8204e-04 - val_loss: 7.0269e-04\n",
            "Epoch 375/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.9864e-04 - val_loss: 6.9763e-04\n",
            "Epoch 376/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7850e-04 - val_loss: 7.3500e-04\n",
            "Epoch 377/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.8697e-04 - val_loss: 6.8471e-04\n",
            "Epoch 378/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.8469e-04 - val_loss: 6.9517e-04\n",
            "Epoch 379/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.6878e-04 - val_loss: 7.2650e-04\n",
            "Epoch 380/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.8909e-04 - val_loss: 6.9529e-04\n",
            "Epoch 381/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.8958e-04 - val_loss: 7.1091e-04\n",
            "Epoch 382/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.9519e-04 - val_loss: 7.8206e-04\n",
            "Epoch 383/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.7777e-04 - val_loss: 6.9756e-04\n",
            "Epoch 384/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.6711e-04 - val_loss: 8.4413e-04\n",
            "Epoch 385/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.7351e-04 - val_loss: 7.0910e-04\n",
            "Epoch 386/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7002e-04 - val_loss: 6.7656e-04\n",
            "Epoch 387/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.7728e-04 - val_loss: 7.1142e-04\n",
            "Epoch 388/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.6543e-04 - val_loss: 7.0345e-04\n",
            "Epoch 389/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7015e-04 - val_loss: 6.9777e-04\n",
            "Epoch 390/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.6869e-04 - val_loss: 7.3009e-04\n",
            "Epoch 391/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.6782e-04 - val_loss: 6.9258e-04\n",
            "Epoch 392/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7266e-04 - val_loss: 7.3759e-04\n",
            "Epoch 393/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7108e-04 - val_loss: 7.8501e-04\n",
            "Epoch 394/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.9548e-04 - val_loss: 6.6375e-04\n",
            "Epoch 395/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7056e-04 - val_loss: 6.8980e-04\n",
            "Epoch 396/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.8204e-04 - val_loss: 7.1754e-04\n",
            "Epoch 397/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.5967e-04 - val_loss: 6.8784e-04\n",
            "Epoch 398/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.6827e-04 - val_loss: 6.5883e-04\n",
            "Epoch 399/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.7091e-04 - val_loss: 7.0082e-04\n",
            "Epoch 400/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6569e-04 - val_loss: 6.9269e-04\n",
            "Epoch 401/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7969e-04 - val_loss: 7.3922e-04\n",
            "Epoch 402/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.6749e-04 - val_loss: 7.2355e-04\n",
            "Epoch 403/1000\n",
            "149/149 [==============================] - 1s 9ms/step - loss: 4.6019e-04 - val_loss: 7.5440e-04\n",
            "Epoch 404/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7730e-04 - val_loss: 6.7458e-04\n",
            "Epoch 405/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6607e-04 - val_loss: 7.1371e-04\n",
            "Epoch 406/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.7581e-04 - val_loss: 6.8352e-04\n",
            "Epoch 407/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6472e-04 - val_loss: 7.1804e-04\n",
            "Epoch 408/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6347e-04 - val_loss: 7.0034e-04\n",
            "Epoch 409/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.5055e-04 - val_loss: 6.9918e-04\n",
            "Epoch 410/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.7531e-04 - val_loss: 6.9286e-04\n",
            "Epoch 411/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.6492e-04 - val_loss: 7.3778e-04\n",
            "Epoch 412/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.9311e-04 - val_loss: 7.4289e-04\n",
            "Epoch 413/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.6952e-04 - val_loss: 6.8645e-04\n",
            "Epoch 414/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.6624e-04 - val_loss: 7.5671e-04\n",
            "Epoch 415/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.5173e-04 - val_loss: 7.0755e-04\n",
            "Epoch 416/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.5593e-04 - val_loss: 7.0818e-04\n",
            "Epoch 417/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.4853e-04 - val_loss: 6.7507e-04\n",
            "Epoch 418/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.5735e-04 - val_loss: 6.9909e-04\n",
            "Epoch 419/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5575e-04 - val_loss: 7.0847e-04\n",
            "Epoch 420/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4581e-04 - val_loss: 6.8567e-04\n",
            "Epoch 421/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4797e-04 - val_loss: 7.1895e-04\n",
            "Epoch 422/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5271e-04 - val_loss: 6.2591e-04\n",
            "Epoch 423/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5471e-04 - val_loss: 7.4177e-04\n",
            "Epoch 424/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5953e-04 - val_loss: 6.6190e-04\n",
            "Epoch 425/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4563e-04 - val_loss: 7.6316e-04\n",
            "Epoch 426/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4894e-04 - val_loss: 6.8192e-04\n",
            "Epoch 427/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5532e-04 - val_loss: 7.1084e-04\n",
            "Epoch 428/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5513e-04 - val_loss: 6.5365e-04\n",
            "Epoch 429/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5467e-04 - val_loss: 7.0841e-04\n",
            "Epoch 430/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6313e-04 - val_loss: 7.2133e-04\n",
            "Epoch 431/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6180e-04 - val_loss: 6.5030e-04\n",
            "Epoch 432/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6839e-04 - val_loss: 7.6900e-04\n",
            "Epoch 433/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4643e-04 - val_loss: 6.8162e-04\n",
            "Epoch 434/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4413e-04 - val_loss: 6.9624e-04\n",
            "Epoch 435/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5967e-04 - val_loss: 6.5124e-04\n",
            "Epoch 436/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4953e-04 - val_loss: 6.7595e-04\n",
            "Epoch 437/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.3436e-04 - val_loss: 6.9088e-04\n",
            "Epoch 438/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.6068e-04 - val_loss: 7.5556e-04\n",
            "Epoch 439/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4284e-04 - val_loss: 6.2085e-04\n",
            "Epoch 440/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3381e-04 - val_loss: 6.4934e-04\n",
            "Epoch 441/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4191e-04 - val_loss: 6.9881e-04\n",
            "Epoch 442/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.4690e-04 - val_loss: 6.3466e-04\n",
            "Epoch 443/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.3982e-04 - val_loss: 7.0252e-04\n",
            "Epoch 444/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.3479e-04 - val_loss: 6.1983e-04\n",
            "Epoch 445/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.6723e-04 - val_loss: 6.4410e-04\n",
            "Epoch 446/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.5264e-04 - val_loss: 6.9115e-04\n",
            "Epoch 447/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.4503e-04 - val_loss: 8.1447e-04\n",
            "Epoch 448/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.5867e-04 - val_loss: 6.0193e-04\n",
            "Epoch 449/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.4411e-04 - val_loss: 6.5111e-04\n",
            "Epoch 450/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.5036e-04 - val_loss: 6.6886e-04\n",
            "Epoch 451/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.3727e-04 - val_loss: 7.3135e-04\n",
            "Epoch 452/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.3595e-04 - val_loss: 7.0096e-04\n",
            "Epoch 453/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2070e-04 - val_loss: 6.7771e-04\n",
            "Epoch 454/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4169e-04 - val_loss: 7.2056e-04\n",
            "Epoch 455/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2437e-04 - val_loss: 6.9307e-04\n",
            "Epoch 456/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4772e-04 - val_loss: 7.4447e-04\n",
            "Epoch 457/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.3282e-04 - val_loss: 6.5101e-04\n",
            "Epoch 458/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.6304e-04 - val_loss: 8.0042e-04\n",
            "Epoch 459/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5984e-04 - val_loss: 6.9493e-04\n",
            "Epoch 460/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3554e-04 - val_loss: 6.4103e-04\n",
            "Epoch 461/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2426e-04 - val_loss: 6.5661e-04\n",
            "Epoch 462/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4650e-04 - val_loss: 7.2490e-04\n",
            "Epoch 463/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4892e-04 - val_loss: 6.2921e-04\n",
            "Epoch 464/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4164e-04 - val_loss: 6.0708e-04\n",
            "Epoch 465/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1977e-04 - val_loss: 6.0280e-04\n",
            "Epoch 466/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3986e-04 - val_loss: 6.0299e-04\n",
            "Epoch 467/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2481e-04 - val_loss: 6.4270e-04\n",
            "Epoch 468/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.5007e-04 - val_loss: 8.3161e-04\n",
            "Epoch 469/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.4912e-04 - val_loss: 6.3971e-04\n",
            "Epoch 470/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1895e-04 - val_loss: 6.6752e-04\n",
            "Epoch 471/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1926e-04 - val_loss: 6.7130e-04\n",
            "Epoch 472/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2991e-04 - val_loss: 7.2242e-04\n",
            "Epoch 473/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2780e-04 - val_loss: 7.0216e-04\n",
            "Epoch 474/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.5159e-04 - val_loss: 6.6392e-04\n",
            "Epoch 475/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3097e-04 - val_loss: 6.1306e-04\n",
            "Epoch 476/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.4813e-04 - val_loss: 6.8487e-04\n",
            "Epoch 477/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.3529e-04 - val_loss: 6.1933e-04\n",
            "Epoch 478/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.3779e-04 - val_loss: 6.2355e-04\n",
            "Epoch 479/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1862e-04 - val_loss: 6.4119e-04\n",
            "Epoch 480/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.1699e-04 - val_loss: 6.2377e-04\n",
            "Epoch 481/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.2767e-04 - val_loss: 6.6085e-04\n",
            "Epoch 482/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3427e-04 - val_loss: 6.7559e-04\n",
            "Epoch 483/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.3952e-04 - val_loss: 6.0815e-04\n",
            "Epoch 484/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.2933e-04 - val_loss: 6.7219e-04\n",
            "Epoch 485/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2277e-04 - val_loss: 6.4175e-04\n",
            "Epoch 486/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2181e-04 - val_loss: 6.9155e-04\n",
            "Epoch 487/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1603e-04 - val_loss: 6.5284e-04\n",
            "Epoch 488/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2456e-04 - val_loss: 6.4024e-04\n",
            "Epoch 489/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2626e-04 - val_loss: 6.8642e-04\n",
            "Epoch 490/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2413e-04 - val_loss: 6.6801e-04\n",
            "Epoch 491/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.4701e-04 - val_loss: 6.2259e-04\n",
            "Epoch 492/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1325e-04 - val_loss: 6.3919e-04\n",
            "Epoch 493/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1459e-04 - val_loss: 6.7832e-04\n",
            "Epoch 494/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3082e-04 - val_loss: 6.9991e-04\n",
            "Epoch 495/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2273e-04 - val_loss: 6.0524e-04\n",
            "Epoch 496/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0527e-04 - val_loss: 6.4881e-04\n",
            "Epoch 497/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.1849e-04 - val_loss: 6.5491e-04\n",
            "Epoch 498/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2855e-04 - val_loss: 6.6401e-04\n",
            "Epoch 499/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2916e-04 - val_loss: 5.9792e-04\n",
            "Epoch 500/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1699e-04 - val_loss: 6.2656e-04\n",
            "Epoch 501/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.3076e-04 - val_loss: 6.3919e-04\n",
            "Epoch 502/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1185e-04 - val_loss: 6.5562e-04\n",
            "Epoch 503/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2209e-04 - val_loss: 6.7395e-04\n",
            "Epoch 504/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1425e-04 - val_loss: 6.4453e-04\n",
            "Epoch 505/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0580e-04 - val_loss: 6.2414e-04\n",
            "Epoch 506/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0805e-04 - val_loss: 7.5556e-04\n",
            "Epoch 507/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.3686e-04 - val_loss: 5.9542e-04\n",
            "Epoch 508/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1781e-04 - val_loss: 7.0169e-04\n",
            "Epoch 509/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0925e-04 - val_loss: 6.4358e-04\n",
            "Epoch 510/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.1580e-04 - val_loss: 7.0678e-04\n",
            "Epoch 511/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.1171e-04 - val_loss: 6.2817e-04\n",
            "Epoch 512/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.2778e-04 - val_loss: 6.7334e-04\n",
            "Epoch 513/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.3561e-04 - val_loss: 6.1229e-04\n",
            "Epoch 514/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.2545e-04 - val_loss: 7.3089e-04\n",
            "Epoch 515/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.1539e-04 - val_loss: 6.0862e-04\n",
            "Epoch 516/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.2225e-04 - val_loss: 5.9471e-04\n",
            "Epoch 517/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.0864e-04 - val_loss: 7.0844e-04\n",
            "Epoch 518/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.1745e-04 - val_loss: 6.2728e-04\n",
            "Epoch 519/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.2254e-04 - val_loss: 6.6087e-04\n",
            "Epoch 520/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0680e-04 - val_loss: 6.8454e-04\n",
            "Epoch 521/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.2343e-04 - val_loss: 5.9392e-04\n",
            "Epoch 522/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.1502e-04 - val_loss: 5.5641e-04\n",
            "Epoch 523/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.1585e-04 - val_loss: 7.1833e-04\n",
            "Epoch 524/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.2125e-04 - val_loss: 5.9634e-04\n",
            "Epoch 525/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1512e-04 - val_loss: 6.1251e-04\n",
            "Epoch 526/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0618e-04 - val_loss: 6.2355e-04\n",
            "Epoch 527/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1725e-04 - val_loss: 7.6296e-04\n",
            "Epoch 528/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2523e-04 - val_loss: 6.0580e-04\n",
            "Epoch 529/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0070e-04 - val_loss: 6.3822e-04\n",
            "Epoch 530/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1529e-04 - val_loss: 5.8885e-04\n",
            "Epoch 531/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0911e-04 - val_loss: 6.6766e-04\n",
            "Epoch 532/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2344e-04 - val_loss: 6.1573e-04\n",
            "Epoch 533/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.1051e-04 - val_loss: 6.4536e-04\n",
            "Epoch 534/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0775e-04 - val_loss: 6.1595e-04\n",
            "Epoch 535/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1340e-04 - val_loss: 6.6178e-04\n",
            "Epoch 536/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.2436e-04 - val_loss: 7.0116e-04\n",
            "Epoch 537/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0990e-04 - val_loss: 6.5984e-04\n",
            "Epoch 538/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0989e-04 - val_loss: 6.7136e-04\n",
            "Epoch 539/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0993e-04 - val_loss: 6.1914e-04\n",
            "Epoch 540/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1180e-04 - val_loss: 6.2602e-04\n",
            "Epoch 541/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0422e-04 - val_loss: 6.6651e-04\n",
            "Epoch 542/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.1030e-04 - val_loss: 5.8118e-04\n",
            "Epoch 543/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8924e-04 - val_loss: 6.0580e-04\n",
            "Epoch 544/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 4.0687e-04 - val_loss: 5.8165e-04\n",
            "Epoch 545/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.9766e-04 - val_loss: 5.9590e-04\n",
            "Epoch 546/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0370e-04 - val_loss: 5.6183e-04\n",
            "Epoch 547/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.2204e-04 - val_loss: 7.2480e-04\n",
            "Epoch 548/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.1333e-04 - val_loss: 6.2909e-04\n",
            "Epoch 549/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0032e-04 - val_loss: 6.0633e-04\n",
            "Epoch 550/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.1264e-04 - val_loss: 6.3924e-04\n",
            "Epoch 551/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.0210e-04 - val_loss: 5.7789e-04\n",
            "Epoch 552/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.1437e-04 - val_loss: 6.2221e-04\n",
            "Epoch 553/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.0392e-04 - val_loss: 6.0376e-04\n",
            "Epoch 554/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 4.1172e-04 - val_loss: 6.3392e-04\n",
            "Epoch 555/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9752e-04 - val_loss: 6.3071e-04\n",
            "Epoch 556/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0723e-04 - val_loss: 5.5871e-04\n",
            "Epoch 557/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0543e-04 - val_loss: 6.1059e-04\n",
            "Epoch 558/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0303e-04 - val_loss: 6.0679e-04\n",
            "Epoch 559/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.9566e-04 - val_loss: 6.4934e-04\n",
            "Epoch 560/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9659e-04 - val_loss: 6.0880e-04\n",
            "Epoch 561/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1955e-04 - val_loss: 6.8968e-04\n",
            "Epoch 562/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0583e-04 - val_loss: 6.5787e-04\n",
            "Epoch 563/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0253e-04 - val_loss: 6.6194e-04\n",
            "Epoch 564/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9924e-04 - val_loss: 5.8114e-04\n",
            "Epoch 565/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9717e-04 - val_loss: 5.8742e-04\n",
            "Epoch 566/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0728e-04 - val_loss: 6.4140e-04\n",
            "Epoch 567/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9735e-04 - val_loss: 5.9204e-04\n",
            "Epoch 568/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9703e-04 - val_loss: 5.8495e-04\n",
            "Epoch 569/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9343e-04 - val_loss: 6.5193e-04\n",
            "Epoch 570/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.1203e-04 - val_loss: 6.1077e-04\n",
            "Epoch 571/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0440e-04 - val_loss: 7.0957e-04\n",
            "Epoch 572/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0037e-04 - val_loss: 6.3805e-04\n",
            "Epoch 573/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9988e-04 - val_loss: 6.6773e-04\n",
            "Epoch 574/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8941e-04 - val_loss: 6.4639e-04\n",
            "Epoch 575/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9893e-04 - val_loss: 5.8909e-04\n",
            "Epoch 576/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9350e-04 - val_loss: 6.3005e-04\n",
            "Epoch 577/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.9154e-04 - val_loss: 5.8725e-04\n",
            "Epoch 578/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9967e-04 - val_loss: 5.7895e-04\n",
            "Epoch 579/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0967e-04 - val_loss: 6.3048e-04\n",
            "Epoch 580/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.8739e-04 - val_loss: 6.6065e-04\n",
            "Epoch 581/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.9440e-04 - val_loss: 5.9233e-04\n",
            "Epoch 582/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.8903e-04 - val_loss: 6.6087e-04\n",
            "Epoch 583/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8904e-04 - val_loss: 5.8798e-04\n",
            "Epoch 584/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.9939e-04 - val_loss: 6.6707e-04\n",
            "Epoch 585/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0056e-04 - val_loss: 7.0264e-04\n",
            "Epoch 586/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8917e-04 - val_loss: 5.9663e-04\n",
            "Epoch 587/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0507e-04 - val_loss: 6.4098e-04\n",
            "Epoch 588/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.9854e-04 - val_loss: 5.6039e-04\n",
            "Epoch 589/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8590e-04 - val_loss: 5.5122e-04\n",
            "Epoch 590/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7804e-04 - val_loss: 6.1270e-04\n",
            "Epoch 591/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9692e-04 - val_loss: 5.9934e-04\n",
            "Epoch 592/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9443e-04 - val_loss: 6.4395e-04\n",
            "Epoch 593/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9934e-04 - val_loss: 6.3629e-04\n",
            "Epoch 594/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.0529e-04 - val_loss: 5.3695e-04\n",
            "Epoch 595/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8360e-04 - val_loss: 6.5993e-04\n",
            "Epoch 596/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9327e-04 - val_loss: 6.7506e-04\n",
            "Epoch 597/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9272e-04 - val_loss: 6.5206e-04\n",
            "Epoch 598/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8578e-04 - val_loss: 5.5041e-04\n",
            "Epoch 599/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9726e-04 - val_loss: 5.6646e-04\n",
            "Epoch 600/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9173e-04 - val_loss: 6.7024e-04\n",
            "Epoch 601/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.8841e-04 - val_loss: 6.6397e-04\n",
            "Epoch 602/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.9068e-04 - val_loss: 6.4128e-04\n",
            "Epoch 603/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.8913e-04 - val_loss: 6.0586e-04\n",
            "Epoch 604/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 4.1112e-04 - val_loss: 6.5341e-04\n",
            "Epoch 605/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.9390e-04 - val_loss: 6.0786e-04\n",
            "Epoch 606/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8656e-04 - val_loss: 6.1987e-04\n",
            "Epoch 607/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 4.0132e-04 - val_loss: 5.7529e-04\n",
            "Epoch 608/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7622e-04 - val_loss: 5.9879e-04\n",
            "Epoch 609/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7643e-04 - val_loss: 6.0170e-04\n",
            "Epoch 610/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7590e-04 - val_loss: 6.2632e-04\n",
            "Epoch 611/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8695e-04 - val_loss: 5.8051e-04\n",
            "Epoch 612/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.8256e-04 - val_loss: 5.9264e-04\n",
            "Epoch 613/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.9201e-04 - val_loss: 6.1548e-04\n",
            "Epoch 614/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.9242e-04 - val_loss: 6.4629e-04\n",
            "Epoch 615/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.8196e-04 - val_loss: 6.4572e-04\n",
            "Epoch 616/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8251e-04 - val_loss: 5.6527e-04\n",
            "Epoch 617/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.9404e-04 - val_loss: 5.9588e-04\n",
            "Epoch 618/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8709e-04 - val_loss: 5.7345e-04\n",
            "Epoch 619/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8406e-04 - val_loss: 6.6451e-04\n",
            "Epoch 620/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.9667e-04 - val_loss: 6.2057e-04\n",
            "Epoch 621/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.8243e-04 - val_loss: 5.7455e-04\n",
            "Epoch 622/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.8126e-04 - val_loss: 6.0766e-04\n",
            "Epoch 623/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8225e-04 - val_loss: 5.5771e-04\n",
            "Epoch 624/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.7078e-04 - val_loss: 5.7449e-04\n",
            "Epoch 625/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7516e-04 - val_loss: 5.4769e-04\n",
            "Epoch 626/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9569e-04 - val_loss: 5.6748e-04\n",
            "Epoch 627/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7863e-04 - val_loss: 5.6954e-04\n",
            "Epoch 628/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7874e-04 - val_loss: 5.7686e-04\n",
            "Epoch 629/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8395e-04 - val_loss: 7.2910e-04\n",
            "Epoch 630/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9946e-04 - val_loss: 6.4885e-04\n",
            "Epoch 631/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8455e-04 - val_loss: 6.4419e-04\n",
            "Epoch 632/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8682e-04 - val_loss: 6.5205e-04\n",
            "Epoch 633/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7769e-04 - val_loss: 6.0569e-04\n",
            "Epoch 634/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8049e-04 - val_loss: 6.4198e-04\n",
            "Epoch 635/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8732e-04 - val_loss: 6.2372e-04\n",
            "Epoch 636/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9840e-04 - val_loss: 5.7619e-04\n",
            "Epoch 637/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8307e-04 - val_loss: 5.5007e-04\n",
            "Epoch 638/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8888e-04 - val_loss: 7.5557e-04\n",
            "Epoch 639/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7714e-04 - val_loss: 5.7732e-04\n",
            "Epoch 640/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8154e-04 - val_loss: 6.2367e-04\n",
            "Epoch 641/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6386e-04 - val_loss: 6.1409e-04\n",
            "Epoch 642/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8515e-04 - val_loss: 6.6450e-04\n",
            "Epoch 643/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7465e-04 - val_loss: 5.6094e-04\n",
            "Epoch 644/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.8594e-04 - val_loss: 5.5866e-04\n",
            "Epoch 645/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.8709e-04 - val_loss: 7.1290e-04\n",
            "Epoch 646/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7608e-04 - val_loss: 6.4607e-04\n",
            "Epoch 647/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.7822e-04 - val_loss: 5.7760e-04\n",
            "Epoch 648/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 4.0175e-04 - val_loss: 5.8107e-04\n",
            "Epoch 649/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.7602e-04 - val_loss: 5.7326e-04\n",
            "Epoch 650/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7162e-04 - val_loss: 5.9589e-04\n",
            "Epoch 651/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8657e-04 - val_loss: 5.3368e-04\n",
            "Epoch 652/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7651e-04 - val_loss: 5.9750e-04\n",
            "Epoch 653/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6933e-04 - val_loss: 5.7947e-04\n",
            "Epoch 654/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7991e-04 - val_loss: 6.5501e-04\n",
            "Epoch 655/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.7068e-04 - val_loss: 5.7289e-04\n",
            "Epoch 656/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.7983e-04 - val_loss: 5.8255e-04\n",
            "Epoch 657/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.7436e-04 - val_loss: 5.9077e-04\n",
            "Epoch 658/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9757e-04 - val_loss: 6.5173e-04\n",
            "Epoch 659/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7282e-04 - val_loss: 6.1765e-04\n",
            "Epoch 660/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8360e-04 - val_loss: 5.7720e-04\n",
            "Epoch 661/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8170e-04 - val_loss: 6.7245e-04\n",
            "Epoch 662/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8184e-04 - val_loss: 6.0916e-04\n",
            "Epoch 663/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7480e-04 - val_loss: 6.0625e-04\n",
            "Epoch 664/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6941e-04 - val_loss: 5.5167e-04\n",
            "Epoch 665/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6518e-04 - val_loss: 5.3889e-04\n",
            "Epoch 666/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8133e-04 - val_loss: 7.0933e-04\n",
            "Epoch 667/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8532e-04 - val_loss: 5.5286e-04\n",
            "Epoch 668/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6217e-04 - val_loss: 6.2794e-04\n",
            "Epoch 669/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8166e-04 - val_loss: 5.5810e-04\n",
            "Epoch 670/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6170e-04 - val_loss: 5.7562e-04\n",
            "Epoch 671/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6602e-04 - val_loss: 5.8531e-04\n",
            "Epoch 672/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6571e-04 - val_loss: 6.1081e-04\n",
            "Epoch 673/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6619e-04 - val_loss: 5.8049e-04\n",
            "Epoch 674/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8209e-04 - val_loss: 6.7873e-04\n",
            "Epoch 675/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7493e-04 - val_loss: 5.7853e-04\n",
            "Epoch 676/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7115e-04 - val_loss: 5.7128e-04\n",
            "Epoch 677/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7946e-04 - val_loss: 5.4866e-04\n",
            "Epoch 678/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7226e-04 - val_loss: 5.2366e-04\n",
            "Epoch 679/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7303e-04 - val_loss: 5.4525e-04\n",
            "Epoch 680/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7976e-04 - val_loss: 6.9975e-04\n",
            "Epoch 681/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.7739e-04 - val_loss: 5.6182e-04\n",
            "Epoch 682/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6305e-04 - val_loss: 5.4377e-04\n",
            "Epoch 683/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.6776e-04 - val_loss: 6.2202e-04\n",
            "Epoch 684/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6195e-04 - val_loss: 6.0159e-04\n",
            "Epoch 685/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6922e-04 - val_loss: 5.2372e-04\n",
            "Epoch 686/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6172e-04 - val_loss: 7.1118e-04\n",
            "Epoch 687/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.7936e-04 - val_loss: 6.6279e-04\n",
            "Epoch 688/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.8180e-04 - val_loss: 5.6394e-04\n",
            "Epoch 689/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6492e-04 - val_loss: 5.4704e-04\n",
            "Epoch 690/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6158e-04 - val_loss: 6.2823e-04\n",
            "Epoch 691/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.7027e-04 - val_loss: 6.3581e-04\n",
            "Epoch 692/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.9148e-04 - val_loss: 5.6787e-04\n",
            "Epoch 693/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6318e-04 - val_loss: 5.2355e-04\n",
            "Epoch 694/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6225e-04 - val_loss: 5.5403e-04\n",
            "Epoch 695/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6757e-04 - val_loss: 5.6302e-04\n",
            "Epoch 696/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6317e-04 - val_loss: 5.9707e-04\n",
            "Epoch 697/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7234e-04 - val_loss: 6.0799e-04\n",
            "Epoch 698/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5797e-04 - val_loss: 5.5366e-04\n",
            "Epoch 699/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6943e-04 - val_loss: 5.9903e-04\n",
            "Epoch 700/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6245e-04 - val_loss: 5.9634e-04\n",
            "Epoch 701/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6796e-04 - val_loss: 6.7968e-04\n",
            "Epoch 702/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7461e-04 - val_loss: 4.9711e-04\n",
            "Epoch 703/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7853e-04 - val_loss: 5.7846e-04\n",
            "Epoch 704/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6214e-04 - val_loss: 5.0135e-04\n",
            "Epoch 705/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5465e-04 - val_loss: 6.8819e-04\n",
            "Epoch 706/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6117e-04 - val_loss: 5.6179e-04\n",
            "Epoch 707/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8061e-04 - val_loss: 6.5424e-04\n",
            "Epoch 708/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.8030e-04 - val_loss: 5.5654e-04\n",
            "Epoch 709/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6964e-04 - val_loss: 6.2213e-04\n",
            "Epoch 710/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5516e-04 - val_loss: 6.2391e-04\n",
            "Epoch 711/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7064e-04 - val_loss: 5.1415e-04\n",
            "Epoch 712/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6340e-04 - val_loss: 5.8665e-04\n",
            "Epoch 713/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6340e-04 - val_loss: 5.4983e-04\n",
            "Epoch 714/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7031e-04 - val_loss: 6.0843e-04\n",
            "Epoch 715/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6351e-04 - val_loss: 5.5669e-04\n",
            "Epoch 716/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.5473e-04 - val_loss: 5.9392e-04\n",
            "Epoch 717/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.7073e-04 - val_loss: 5.1602e-04\n",
            "Epoch 718/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6792e-04 - val_loss: 6.0756e-04\n",
            "Epoch 719/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.7092e-04 - val_loss: 5.3443e-04\n",
            "Epoch 720/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6194e-04 - val_loss: 5.2183e-04\n",
            "Epoch 721/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6593e-04 - val_loss: 5.9322e-04\n",
            "Epoch 722/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.5804e-04 - val_loss: 5.7532e-04\n",
            "Epoch 723/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6249e-04 - val_loss: 5.7804e-04\n",
            "Epoch 724/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6029e-04 - val_loss: 6.1865e-04\n",
            "Epoch 725/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6842e-04 - val_loss: 5.2286e-04\n",
            "Epoch 726/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4908e-04 - val_loss: 6.1417e-04\n",
            "Epoch 727/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5585e-04 - val_loss: 6.1005e-04\n",
            "Epoch 728/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.7812e-04 - val_loss: 5.8744e-04\n",
            "Epoch 729/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5701e-04 - val_loss: 5.4911e-04\n",
            "Epoch 730/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6275e-04 - val_loss: 5.3438e-04\n",
            "Epoch 731/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.5153e-04 - val_loss: 5.9976e-04\n",
            "Epoch 732/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6754e-04 - val_loss: 6.1627e-04\n",
            "Epoch 733/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5232e-04 - val_loss: 5.7341e-04\n",
            "Epoch 734/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6263e-04 - val_loss: 5.8393e-04\n",
            "Epoch 735/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6975e-04 - val_loss: 6.4667e-04\n",
            "Epoch 736/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5817e-04 - val_loss: 6.0886e-04\n",
            "Epoch 737/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4604e-04 - val_loss: 5.3813e-04\n",
            "Epoch 738/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5328e-04 - val_loss: 5.4280e-04\n",
            "Epoch 739/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5769e-04 - val_loss: 6.5688e-04\n",
            "Epoch 740/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5037e-04 - val_loss: 5.5726e-04\n",
            "Epoch 741/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6055e-04 - val_loss: 5.5045e-04\n",
            "Epoch 742/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6425e-04 - val_loss: 5.7154e-04\n",
            "Epoch 743/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5501e-04 - val_loss: 5.4129e-04\n",
            "Epoch 744/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6199e-04 - val_loss: 6.9291e-04\n",
            "Epoch 745/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5108e-04 - val_loss: 5.8794e-04\n",
            "Epoch 746/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6279e-04 - val_loss: 4.9538e-04\n",
            "Epoch 747/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6091e-04 - val_loss: 5.4381e-04\n",
            "Epoch 748/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6453e-04 - val_loss: 5.1277e-04\n",
            "Epoch 749/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6096e-04 - val_loss: 5.6627e-04\n",
            "Epoch 750/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5678e-04 - val_loss: 5.9112e-04\n",
            "Epoch 751/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.5596e-04 - val_loss: 5.6040e-04\n",
            "Epoch 752/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.5602e-04 - val_loss: 6.0839e-04\n",
            "Epoch 753/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6272e-04 - val_loss: 5.6230e-04\n",
            "Epoch 754/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.7202e-04 - val_loss: 6.1335e-04\n",
            "Epoch 755/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6466e-04 - val_loss: 5.2827e-04\n",
            "Epoch 756/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5580e-04 - val_loss: 6.2071e-04\n",
            "Epoch 757/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.6805e-04 - val_loss: 5.3399e-04\n",
            "Epoch 758/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4662e-04 - val_loss: 5.0652e-04\n",
            "Epoch 759/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.4980e-04 - val_loss: 5.9408e-04\n",
            "Epoch 760/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.5443e-04 - val_loss: 6.0955e-04\n",
            "Epoch 761/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.5393e-04 - val_loss: 6.2920e-04\n",
            "Epoch 762/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.6076e-04 - val_loss: 5.7778e-04\n",
            "Epoch 763/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5495e-04 - val_loss: 5.0623e-04\n",
            "Epoch 764/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5433e-04 - val_loss: 6.2123e-04\n",
            "Epoch 765/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5388e-04 - val_loss: 5.8176e-04\n",
            "Epoch 766/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6064e-04 - val_loss: 5.7531e-04\n",
            "Epoch 767/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4770e-04 - val_loss: 6.1526e-04\n",
            "Epoch 768/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5008e-04 - val_loss: 5.6849e-04\n",
            "Epoch 769/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5353e-04 - val_loss: 6.0147e-04\n",
            "Epoch 770/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6772e-04 - val_loss: 5.6804e-04\n",
            "Epoch 771/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5282e-04 - val_loss: 5.5278e-04\n",
            "Epoch 772/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4889e-04 - val_loss: 6.2979e-04\n",
            "Epoch 773/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6409e-04 - val_loss: 6.2414e-04\n",
            "Epoch 774/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6082e-04 - val_loss: 6.0243e-04\n",
            "Epoch 775/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6316e-04 - val_loss: 5.9734e-04\n",
            "Epoch 776/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4351e-04 - val_loss: 5.5202e-04\n",
            "Epoch 777/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4496e-04 - val_loss: 5.5641e-04\n",
            "Epoch 778/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4104e-04 - val_loss: 5.5811e-04\n",
            "Epoch 779/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.5360e-04 - val_loss: 5.9154e-04\n",
            "Epoch 780/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5106e-04 - val_loss: 5.5948e-04\n",
            "Epoch 781/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5491e-04 - val_loss: 5.4773e-04\n",
            "Epoch 782/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6192e-04 - val_loss: 6.1196e-04\n",
            "Epoch 783/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5219e-04 - val_loss: 5.0880e-04\n",
            "Epoch 784/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.6858e-04 - val_loss: 5.9918e-04\n",
            "Epoch 785/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.7031e-04 - val_loss: 5.1134e-04\n",
            "Epoch 786/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.5407e-04 - val_loss: 5.4185e-04\n",
            "Epoch 787/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.5113e-04 - val_loss: 5.6385e-04\n",
            "Epoch 788/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.5240e-04 - val_loss: 5.7883e-04\n",
            "Epoch 789/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.4199e-04 - val_loss: 5.0589e-04\n",
            "Epoch 790/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.4491e-04 - val_loss: 6.2437e-04\n",
            "Epoch 791/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3526e-04 - val_loss: 6.1980e-04\n",
            "Epoch 792/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5693e-04 - val_loss: 6.1779e-04\n",
            "Epoch 793/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4309e-04 - val_loss: 4.8750e-04\n",
            "Epoch 794/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.4417e-04 - val_loss: 5.5310e-04\n",
            "Epoch 795/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.4632e-04 - val_loss: 6.3857e-04\n",
            "Epoch 796/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5733e-04 - val_loss: 5.7204e-04\n",
            "Epoch 797/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5206e-04 - val_loss: 6.2884e-04\n",
            "Epoch 798/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4154e-04 - val_loss: 5.9976e-04\n",
            "Epoch 799/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.5304e-04 - val_loss: 6.0587e-04\n",
            "Epoch 800/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4025e-04 - val_loss: 6.1047e-04\n",
            "Epoch 801/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5575e-04 - val_loss: 5.8194e-04\n",
            "Epoch 802/1000\n",
            "149/149 [==============================] - 0s 2ms/step - loss: 3.6018e-04 - val_loss: 5.7694e-04\n",
            "Epoch 803/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4260e-04 - val_loss: 5.4206e-04\n",
            "Epoch 804/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5803e-04 - val_loss: 5.9790e-04\n",
            "Epoch 805/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3464e-04 - val_loss: 4.8299e-04\n",
            "Epoch 806/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4156e-04 - val_loss: 5.2062e-04\n",
            "Epoch 807/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4942e-04 - val_loss: 5.8584e-04\n",
            "Epoch 808/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4515e-04 - val_loss: 5.8977e-04\n",
            "Epoch 809/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5316e-04 - val_loss: 5.5423e-04\n",
            "Epoch 810/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.4882e-04 - val_loss: 5.7793e-04\n",
            "Epoch 811/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4769e-04 - val_loss: 5.6192e-04\n",
            "Epoch 812/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3931e-04 - val_loss: 5.7829e-04\n",
            "Epoch 813/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3917e-04 - val_loss: 5.9218e-04\n",
            "Epoch 814/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4379e-04 - val_loss: 5.6128e-04\n",
            "Epoch 815/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.5442e-04 - val_loss: 5.4248e-04\n",
            "Epoch 816/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4866e-04 - val_loss: 5.7072e-04\n",
            "Epoch 817/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3856e-04 - val_loss: 5.6498e-04\n",
            "Epoch 818/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.4876e-04 - val_loss: 5.6645e-04\n",
            "Epoch 819/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3662e-04 - val_loss: 5.9285e-04\n",
            "Epoch 820/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.5393e-04 - val_loss: 4.8763e-04\n",
            "Epoch 821/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3162e-04 - val_loss: 5.2865e-04\n",
            "Epoch 822/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.4934e-04 - val_loss: 5.8737e-04\n",
            "Epoch 823/1000\n",
            "149/149 [==============================] - 1s 6ms/step - loss: 3.4515e-04 - val_loss: 6.0706e-04\n",
            "Epoch 824/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3737e-04 - val_loss: 5.7000e-04\n",
            "Epoch 825/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3078e-04 - val_loss: 4.8874e-04\n",
            "Epoch 826/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.3670e-04 - val_loss: 4.9962e-04\n",
            "Epoch 827/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2814e-04 - val_loss: 5.8012e-04\n",
            "Epoch 828/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3728e-04 - val_loss: 4.8449e-04\n",
            "Epoch 829/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4305e-04 - val_loss: 4.9290e-04\n",
            "Epoch 830/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3450e-04 - val_loss: 5.1203e-04\n",
            "Epoch 831/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3594e-04 - val_loss: 5.8867e-04\n",
            "Epoch 832/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3821e-04 - val_loss: 5.2184e-04\n",
            "Epoch 833/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4700e-04 - val_loss: 4.7209e-04\n",
            "Epoch 834/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4182e-04 - val_loss: 5.0741e-04\n",
            "Epoch 835/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3739e-04 - val_loss: 5.0907e-04\n",
            "Epoch 836/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3263e-04 - val_loss: 5.3356e-04\n",
            "Epoch 837/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4936e-04 - val_loss: 5.1679e-04\n",
            "Epoch 838/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4171e-04 - val_loss: 6.2260e-04\n",
            "Epoch 839/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4940e-04 - val_loss: 5.4347e-04\n",
            "Epoch 840/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3086e-04 - val_loss: 5.1724e-04\n",
            "Epoch 841/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4351e-04 - val_loss: 4.8309e-04\n",
            "Epoch 842/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4616e-04 - val_loss: 5.0608e-04\n",
            "Epoch 843/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4177e-04 - val_loss: 5.1170e-04\n",
            "Epoch 844/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4508e-04 - val_loss: 5.9669e-04\n",
            "Epoch 845/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4835e-04 - val_loss: 5.6807e-04\n",
            "Epoch 846/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3175e-04 - val_loss: 5.0640e-04\n",
            "Epoch 847/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3565e-04 - val_loss: 4.6503e-04\n",
            "Epoch 848/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3374e-04 - val_loss: 5.0003e-04\n",
            "Epoch 849/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3080e-04 - val_loss: 5.5299e-04\n",
            "Epoch 850/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4140e-04 - val_loss: 5.9048e-04\n",
            "Epoch 851/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3810e-04 - val_loss: 5.5967e-04\n",
            "Epoch 852/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.5660e-04 - val_loss: 5.5754e-04\n",
            "Epoch 853/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3588e-04 - val_loss: 5.9414e-04\n",
            "Epoch 854/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3835e-04 - val_loss: 5.3390e-04\n",
            "Epoch 855/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3021e-04 - val_loss: 5.3225e-04\n",
            "Epoch 856/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2533e-04 - val_loss: 5.8109e-04\n",
            "Epoch 857/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.3327e-04 - val_loss: 5.8674e-04\n",
            "Epoch 858/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2765e-04 - val_loss: 4.7772e-04\n",
            "Epoch 859/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.4541e-04 - val_loss: 5.4024e-04\n",
            "Epoch 860/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3504e-04 - val_loss: 5.1385e-04\n",
            "Epoch 861/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3314e-04 - val_loss: 5.8089e-04\n",
            "Epoch 862/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.4558e-04 - val_loss: 5.1960e-04\n",
            "Epoch 863/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3782e-04 - val_loss: 5.7523e-04\n",
            "Epoch 864/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3162e-04 - val_loss: 5.1059e-04\n",
            "Epoch 865/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3438e-04 - val_loss: 6.1165e-04\n",
            "Epoch 866/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3800e-04 - val_loss: 5.3707e-04\n",
            "Epoch 867/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3636e-04 - val_loss: 5.2226e-04\n",
            "Epoch 868/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3212e-04 - val_loss: 5.1866e-04\n",
            "Epoch 869/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3443e-04 - val_loss: 6.1214e-04\n",
            "Epoch 870/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4515e-04 - val_loss: 4.7121e-04\n",
            "Epoch 871/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4511e-04 - val_loss: 5.5112e-04\n",
            "Epoch 872/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2716e-04 - val_loss: 5.9799e-04\n",
            "Epoch 873/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3590e-04 - val_loss: 5.2955e-04\n",
            "Epoch 874/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2717e-04 - val_loss: 5.2370e-04\n",
            "Epoch 875/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3664e-04 - val_loss: 4.9397e-04\n",
            "Epoch 876/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3515e-04 - val_loss: 4.8498e-04\n",
            "Epoch 877/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3500e-04 - val_loss: 5.5227e-04\n",
            "Epoch 878/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3110e-04 - val_loss: 5.5696e-04\n",
            "Epoch 879/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3896e-04 - val_loss: 4.5330e-04\n",
            "Epoch 880/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2605e-04 - val_loss: 4.9878e-04\n",
            "Epoch 881/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2652e-04 - val_loss: 4.3211e-04\n",
            "Epoch 882/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3887e-04 - val_loss: 5.7392e-04\n",
            "Epoch 883/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4048e-04 - val_loss: 5.3255e-04\n",
            "Epoch 884/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.2741e-04 - val_loss: 5.4657e-04\n",
            "Epoch 885/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4002e-04 - val_loss: 6.1156e-04\n",
            "Epoch 886/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3388e-04 - val_loss: 5.3852e-04\n",
            "Epoch 887/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.4454e-04 - val_loss: 5.3918e-04\n",
            "Epoch 888/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3991e-04 - val_loss: 5.3683e-04\n",
            "Epoch 889/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.4280e-04 - val_loss: 5.7656e-04\n",
            "Epoch 890/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.3809e-04 - val_loss: 5.2937e-04\n",
            "Epoch 891/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2200e-04 - val_loss: 4.6624e-04\n",
            "Epoch 892/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.3611e-04 - val_loss: 5.5931e-04\n",
            "Epoch 893/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.4377e-04 - val_loss: 5.6338e-04\n",
            "Epoch 894/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2941e-04 - val_loss: 5.5414e-04\n",
            "Epoch 895/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1927e-04 - val_loss: 5.1958e-04\n",
            "Epoch 896/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3828e-04 - val_loss: 5.5007e-04\n",
            "Epoch 897/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3175e-04 - val_loss: 6.8209e-04\n",
            "Epoch 898/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.4713e-04 - val_loss: 5.5622e-04\n",
            "Epoch 899/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1433e-04 - val_loss: 5.1773e-04\n",
            "Epoch 900/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2733e-04 - val_loss: 5.7337e-04\n",
            "Epoch 901/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2512e-04 - val_loss: 5.9477e-04\n",
            "Epoch 902/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2572e-04 - val_loss: 5.5069e-04\n",
            "Epoch 903/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2361e-04 - val_loss: 5.6119e-04\n",
            "Epoch 904/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.4217e-04 - val_loss: 5.8100e-04\n",
            "Epoch 905/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2163e-04 - val_loss: 6.0923e-04\n",
            "Epoch 906/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3365e-04 - val_loss: 4.7918e-04\n",
            "Epoch 907/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4034e-04 - val_loss: 5.7436e-04\n",
            "Epoch 908/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3145e-04 - val_loss: 5.1811e-04\n",
            "Epoch 909/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3069e-04 - val_loss: 5.9524e-04\n",
            "Epoch 910/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2361e-04 - val_loss: 5.5717e-04\n",
            "Epoch 911/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2476e-04 - val_loss: 5.1717e-04\n",
            "Epoch 912/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2490e-04 - val_loss: 4.8484e-04\n",
            "Epoch 913/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.4055e-04 - val_loss: 5.6753e-04\n",
            "Epoch 914/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2297e-04 - val_loss: 5.0188e-04\n",
            "Epoch 915/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2856e-04 - val_loss: 5.6422e-04\n",
            "Epoch 916/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2543e-04 - val_loss: 5.1255e-04\n",
            "Epoch 917/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2307e-04 - val_loss: 6.0529e-04\n",
            "Epoch 918/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2217e-04 - val_loss: 5.3907e-04\n",
            "Epoch 919/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1678e-04 - val_loss: 5.3685e-04\n",
            "Epoch 920/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2664e-04 - val_loss: 4.9008e-04\n",
            "Epoch 921/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3482e-04 - val_loss: 5.7992e-04\n",
            "Epoch 922/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2342e-04 - val_loss: 6.2929e-04\n",
            "Epoch 923/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2723e-04 - val_loss: 5.7543e-04\n",
            "Epoch 924/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2446e-04 - val_loss: 5.9165e-04\n",
            "Epoch 925/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2990e-04 - val_loss: 5.8227e-04\n",
            "Epoch 926/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3000e-04 - val_loss: 4.7496e-04\n",
            "Epoch 927/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2382e-04 - val_loss: 5.1186e-04\n",
            "Epoch 928/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3415e-04 - val_loss: 5.2153e-04\n",
            "Epoch 929/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2797e-04 - val_loss: 5.3055e-04\n",
            "Epoch 930/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.3690e-04 - val_loss: 5.6427e-04\n",
            "Epoch 931/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.1854e-04 - val_loss: 5.7905e-04\n",
            "Epoch 932/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1790e-04 - val_loss: 6.1171e-04\n",
            "Epoch 933/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2166e-04 - val_loss: 4.7197e-04\n",
            "Epoch 934/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1849e-04 - val_loss: 5.0308e-04\n",
            "Epoch 935/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2069e-04 - val_loss: 4.9209e-04\n",
            "Epoch 936/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3473e-04 - val_loss: 5.5293e-04\n",
            "Epoch 937/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3623e-04 - val_loss: 4.7127e-04\n",
            "Epoch 938/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2021e-04 - val_loss: 5.1485e-04\n",
            "Epoch 939/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1971e-04 - val_loss: 4.5642e-04\n",
            "Epoch 940/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2356e-04 - val_loss: 5.4172e-04\n",
            "Epoch 941/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2258e-04 - val_loss: 5.2018e-04\n",
            "Epoch 942/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2505e-04 - val_loss: 5.0563e-04\n",
            "Epoch 943/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1673e-04 - val_loss: 5.8320e-04\n",
            "Epoch 944/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1589e-04 - val_loss: 4.5043e-04\n",
            "Epoch 945/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2050e-04 - val_loss: 4.5093e-04\n",
            "Epoch 946/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2918e-04 - val_loss: 5.3791e-04\n",
            "Epoch 947/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.3453e-04 - val_loss: 5.2241e-04\n",
            "Epoch 948/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1664e-04 - val_loss: 5.4058e-04\n",
            "Epoch 949/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1342e-04 - val_loss: 5.5986e-04\n",
            "Epoch 950/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2205e-04 - val_loss: 5.5929e-04\n",
            "Epoch 951/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2143e-04 - val_loss: 5.6205e-04\n",
            "Epoch 952/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1438e-04 - val_loss: 4.7611e-04\n",
            "Epoch 953/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1703e-04 - val_loss: 4.8343e-04\n",
            "Epoch 954/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2775e-04 - val_loss: 5.0208e-04\n",
            "Epoch 955/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.1306e-04 - val_loss: 5.4920e-04\n",
            "Epoch 956/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2642e-04 - val_loss: 5.6788e-04\n",
            "Epoch 957/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2990e-04 - val_loss: 4.8266e-04\n",
            "Epoch 958/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1752e-04 - val_loss: 5.1802e-04\n",
            "Epoch 959/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.0991e-04 - val_loss: 4.8926e-04\n",
            "Epoch 960/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2457e-04 - val_loss: 4.6076e-04\n",
            "Epoch 961/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2690e-04 - val_loss: 5.9128e-04\n",
            "Epoch 962/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2458e-04 - val_loss: 4.7769e-04\n",
            "Epoch 963/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0969e-04 - val_loss: 5.7162e-04\n",
            "Epoch 964/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1313e-04 - val_loss: 5.8615e-04\n",
            "Epoch 965/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.2770e-04 - val_loss: 5.2507e-04\n",
            "Epoch 966/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1296e-04 - val_loss: 5.7434e-04\n",
            "Epoch 967/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1589e-04 - val_loss: 5.3137e-04\n",
            "Epoch 968/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1616e-04 - val_loss: 5.5006e-04\n",
            "Epoch 969/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.2781e-04 - val_loss: 4.4572e-04\n",
            "Epoch 970/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1366e-04 - val_loss: 5.3529e-04\n",
            "Epoch 971/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1168e-04 - val_loss: 5.3431e-04\n",
            "Epoch 972/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2245e-04 - val_loss: 5.5269e-04\n",
            "Epoch 973/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0896e-04 - val_loss: 4.7034e-04\n",
            "Epoch 974/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1266e-04 - val_loss: 4.8488e-04\n",
            "Epoch 975/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.2065e-04 - val_loss: 5.6221e-04\n",
            "Epoch 976/1000\n",
            "149/149 [==============================] - 1s 5ms/step - loss: 3.1491e-04 - val_loss: 5.6422e-04\n",
            "Epoch 977/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1431e-04 - val_loss: 4.6854e-04\n",
            "Epoch 978/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1266e-04 - val_loss: 5.0671e-04\n",
            "Epoch 979/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1568e-04 - val_loss: 4.8222e-04\n",
            "Epoch 980/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.0244e-04 - val_loss: 5.4265e-04\n",
            "Epoch 981/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1856e-04 - val_loss: 5.0047e-04\n",
            "Epoch 982/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1318e-04 - val_loss: 5.4043e-04\n",
            "Epoch 983/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2168e-04 - val_loss: 5.9594e-04\n",
            "Epoch 984/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2022e-04 - val_loss: 4.9551e-04\n",
            "Epoch 985/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1738e-04 - val_loss: 5.0359e-04\n",
            "Epoch 986/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.1362e-04 - val_loss: 5.1814e-04\n",
            "Epoch 987/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.1929e-04 - val_loss: 5.4897e-04\n",
            "Epoch 988/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.0609e-04 - val_loss: 4.9380e-04\n",
            "Epoch 989/1000\n",
            "149/149 [==============================] - 0s 3ms/step - loss: 3.2781e-04 - val_loss: 4.3655e-04\n",
            "Epoch 990/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1729e-04 - val_loss: 5.5252e-04\n",
            "Epoch 991/1000\n",
            "149/149 [==============================] - 1s 3ms/step - loss: 3.0681e-04 - val_loss: 5.6360e-04\n",
            "Epoch 992/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0699e-04 - val_loss: 4.8103e-04\n",
            "Epoch 993/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0520e-04 - val_loss: 5.1324e-04\n",
            "Epoch 994/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1308e-04 - val_loss: 5.6318e-04\n",
            "Epoch 995/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0830e-04 - val_loss: 4.9305e-04\n",
            "Epoch 996/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1056e-04 - val_loss: 4.9640e-04\n",
            "Epoch 997/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1646e-04 - val_loss: 5.1260e-04\n",
            "Epoch 998/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.1292e-04 - val_loss: 5.2682e-04\n",
            "Epoch 999/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0620e-04 - val_loss: 4.8848e-04\n",
            "Epoch 1000/1000\n",
            "149/149 [==============================] - 1s 4ms/step - loss: 3.0670e-04 - val_loss: 4.9587e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate metrics based on SymSeekerNN class"
      ],
      "metadata": {
        "id": "t8--9GP4CQTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model (if previously saved in a file)"
      ],
      "metadata": {
        "id": "nFsZTe2_Hcfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "problem_name = str(n)+'root'\n",
        "Nfeatures=2\n",
        "Nbranches=3\n",
        "sim=0\n",
        "path='./models_'+problem_name+'/Model_'+str(sim)+'sim_'+str(Nbranches)+'branches'\n",
        "model=SymSeekerNN.load_model(path,Nfeatures,Nbranches)\n",
        "model.compile(optimizer='adam')\n"
      ],
      "metadata": {
        "id": "eOpAkE2PcAG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative way ...\n",
        "print('path=',path)\n",
        "model_saved=models.load_model(path,compile=False)\n",
        "model=SymSeekerNN(Nfeatures=Nfeatures,\n",
        "                  Nbranches=Nbranches,\n",
        "                  model_saved=model_saved)\n",
        "model.compile(optimizer='adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut7cCgLIk_RS",
        "outputId": "1ce7505d-02b9-498c-b187-059ee723dbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path= ./models_3root/Model_0sim_3branches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction of $x$"
      ],
      "metadata": {
        "id": "-8ApMon8ygR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xtr = np.tile(xtr_single,reps=(1,Nbranches))\n",
        "\n",
        "model.summary()\n",
        "xtr_NN = model.predict(ytr)\n",
        "score=model.evaluate_loss(xtr,xtr_NN)\n",
        "print(\"Loss=\",score.numpy())\n",
        "print(xtr_NN.shape,ytr.shape)\n",
        "\n",
        "true_data=False\n",
        "if(true_data):\n",
        "  x=xtr;Nb=1\n",
        "else:\n",
        "  x=xtr_NN;Nb=Nbranches\n",
        "\n",
        "print(xtr.shape)\n",
        "for i in range(Nb):\n",
        "  print(i)\n",
        "  # Sample data\n",
        "  x1 = x[:,2*i]\n",
        "  x2 = x[:,2*i+1]\n",
        "  # Create a scatter plot\n",
        "  plt.scatter(x1, x2)\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title('SSNN prediction')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Set equal aspect ratio for the plot\n",
        "plt.axis('square')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "wnnYvzfYzEnR",
        "outputId": "dbd47348-216a-4c86-a2fa-7a8c63218447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_1 (Sequential)   (None, 2)                 162       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 6)                 12        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174\n",
            "Trainable params: 174\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "594/594 [==============================] - 4s 7ms/step\n",
            "Loss= 0.0003031304\n",
            "(19000, 6) (19000, 2)\n",
            "(19000, 6)\n",
            "0\n",
            "1\n",
            "2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate $Activity$"
      ],
      "metadata": {
        "id": "Ij-gJC7G3wHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(Nbranches):\n",
        " activity=model.activity_single_branch(i,xtr,xtr_NN)\n",
        " print(\"Activity=\",activity)\n",
        "\n",
        "activities=   model.list_activities(xtr,xtr_NN)\n",
        "print(\"\\n List of activities=\",activities)\n",
        "\n",
        "index_reference=np.argmax(activities)\n",
        "print(\"\\n Index of reference=\",index_reference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htbVRX2L3wMO",
        "outputId": "8adf7704-20ae-40c0-8c88-cb2a8fb3b267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activity= 33.22105263157895\n",
            "Activity= 33.705263157894734\n",
            "Activity= 33.07368421052632\n",
            "\n",
            " List of activities= [33.22105263157895, 33.705263157894734, 33.07368421052632]\n",
            "\n",
            " Index of reference= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate group metrics: $d_{closed}$ and $d_{inverse}$"
      ],
      "metadata": {
        "id": "khYUEc5W3tgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_NN=model.obtain_list_branch_matrices_ref(index_reference)\n",
        "#print(\"List mat=\\n\",D_NN)\n",
        "\n",
        "# Print the array in a human-readable format\n",
        "for i,arr in enumerate(D_NN):\n",
        "  print(\"D=\",i)\n",
        "  np.savetxt(sys.stdout, arr, fmt='%f')\n",
        "\n",
        "print(\"\\n d_closed metrics=\",model.calculate_d_closed(index_reference))\n",
        "print(\" d_inverse metrics=\",model.calculate_d_inv(index_reference))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8aVyLbcCUh2",
        "outputId": "969df499-d587-4e35-a5ee-41ee6c4b1ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D= 0\n",
            "-0.510816 0.875265\n",
            "-0.871557 -0.489350\n",
            "D= 1\n",
            "1.000000 0.000000\n",
            "-0.000000 1.000000\n",
            "D= 2\n",
            "-0.500721 -0.874779\n",
            "0.866983 -0.507413\n",
            "\n",
            " d_closed metrics= 0.003408576469379396\n",
            " d_inverse metrics= 0.006588328654116775\n"
          ]
        }
      ]
    }
  ]
}